{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc6c7873-5841-4927-ad07-c4d9c9643b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import pyArango\n",
    "import os\n",
    "from os import path\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "from arango import ArangoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102ea8a7-7052-4641-af64-9fea04316dff",
   "metadata": {},
   "source": [
    "Initialisation de la connection avec la base de données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7232caf-3eff-4b8b-bd0e-56ba3bc44a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the client for ArangoDB.\n",
    "client = ArangoClient(hosts=\"http://localhost:8529\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cb3f359-7e8e-49bf-87a4-9fd3b76a11ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_db = client.db('_system', username='root',password='root')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987f104-3e32-4c55-9e8c-6ea4f499644e",
   "metadata": {},
   "source": [
    "si besoin de repartir à zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "65c6106a-66bb-47dd-a771-5881e9e203c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_db.delete_database('text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5bb57b-0e9e-411b-8cc1-502108436582",
   "metadata": {},
   "source": [
    "Check si la base de données existe déjà et création si besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a20a9e06-ed07-4f5b-8b47-00d6041d6040",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not sys_db.has_database('text'):\n",
    "    \n",
    "    sys_db.create_database('text')\n",
    "    db = client.db(\"text\", username=\"root\", password=\"root\")\n",
    "    graph = db.create_graph('text_explorer')\n",
    "    # création des collections\n",
    "    docs = graph.create_vertex_collection('docs')\n",
    "    sentences = graph.create_vertex_collection('sentences')\n",
    "    tokens = graph.create_vertex_collection('tokens')\n",
    "    lemmas = graph.create_vertex_collection('lemmas')\n",
    "    # création des arrêtes\n",
    "    is_from = graph.create_edge_definition(\n",
    "        edge_collection='is_from',\n",
    "        from_vertex_collections=['sentences','tokens'],\n",
    "        to_vertex_collections=['docs','sentences']\n",
    "    )\n",
    "    contracts_to = graph.create_edge_definition(\n",
    "        edge_collection='contracts_to',\n",
    "        from_vertex_collections=['tokens'],\n",
    "        to_vertex_collections=['lemmas']\n",
    "    )\n",
    "    syntagmatic_link = graph.create_edge_definition(\n",
    "        edge_collection='syntagmatic_link',\n",
    "        from_vertex_collections=['tokens'],\n",
    "        to_vertex_collections=['tokens']\n",
    "    )\n",
    "else:\n",
    "    db = client.db(\"text\", username=\"root\", password=\"root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57df3b3-4e03-4e6b-9347-a372fd5f0638",
   "metadata": {},
   "source": [
    "L'objectif maintenant est de remplir ces champs avec les données extraites depuis le texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e35a3-c60c-44eb-ab33-aef3f046f55b",
   "metadata": {},
   "source": [
    "Récupération des documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551cdde1-7d69-4e98-9c99-193778339df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(path):\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        return(f.read().replace('\\n',' '))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e859a1b1-d3d1-4064-b33e-30bf440d5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir_path = os.getcwd()\n",
    "#dir_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d540b4d-726e-4254-bf5a-719c699d4251",
   "metadata": {},
   "source": [
    "Liste des fichiers présents dans le dossier choisi pour l'analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "405e5599-89c9-414b-8e90-8aa71d951494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/paul/projects/text_for_app/jean_blog.txt',\n",
       " '/home/paul/projects/text_for_app/emploi étudiant et inégalités sociales.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('/home/paul/projects/text_for_app/*.{}'.format('txt'))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e6ef6cb5-0757-4017-a3b8-d14be62a678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_from_path(path):\n",
    "    return os.path.normpath(path).split(os.sep)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "490f3921-3654-4653-a4c4-3b59bee566a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>doc_name</th>\n",
       "      <th>doc_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/paul/projects/text_for_app/jean_blog.txt</td>\n",
       "      <td>jean_blog.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/paul/projects/text_for_app/emploi étudia...</td>\n",
       "      <td>emploi étudiant et inégalités sociales.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath  \\\n",
       "0     /home/paul/projects/text_for_app/jean_blog.txt   \n",
       "1  /home/paul/projects/text_for_app/emploi étudia...   \n",
       "\n",
       "                                     doc_name  doc_number  \n",
       "0                               jean_blog.txt           0  \n",
       "1  emploi étudiant et inégalités sociales.txt           1  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = pd.DataFrame({'filepath':files,\n",
    "                          'doc_name':[get_filename_from_path(filepath) for filepath in files],\n",
    "                          'doc_number':list(range(0,len(files)))})\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "11549c59-b792-4492-b26b-be12ac62163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_docs(documents_found):\n",
    "    in_db_doclist = pd.DataFrame(list(db.aql.execute('''FOR doc in docs RETURN doc''')))\n",
    "    if in_db_doclist.shape == (0,0):\n",
    "        pass\n",
    "    else :\n",
    "        documents_found = documents_found[~documents_found['doc_name'].isin(in_db_doclist['doc_name'])]\n",
    "        last_doc_number = in_db_doclist['doc_number'].max()\n",
    "        documents_found['doc_number'] = documents_found['doc_number'] + last_doc_number + 1\n",
    "    \n",
    "    dict_list_documents_to_insert = []\n",
    "    for doc_name, number, path  in zip(documents_found['doc_name'], documents_found['doc_number'], documents_found['filepath']):\n",
    "        dict_list_documents_to_insert.append({'_key':f'doc{number}',\n",
    "                                                'doc_name':doc_name,\n",
    "                                                'doc_path':path,\n",
    "                                                'doc_number':number,\n",
    "                                                'processed':'False'})\n",
    "    db.collection('docs').import_bulk(dict_list_documents_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "72c053c1-169a-443b-a1c3-1d55fdf44a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_docs(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b5745-ffc1-475f-a3f7-b8bf2e4efffe",
   "metadata": {},
   "source": [
    "# Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "445a2ee3-c54b-4013-9c62-47687b08ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dfbdeb-8e90-4c45-bf06-3855041d95eb",
   "metadata": {},
   "source": [
    "Traitement des fichiers par le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4464a67-236a-497a-ae38-c562907e975d",
   "metadata": {},
   "source": [
    "## se renseigner sur comment faire du traitement en batch\n",
    "    - mesurer l'empreinte mémoire de l'opération et ajuster combien de documents en même temps peuvent être traités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4986fa9-3661-4abb-bfe8-9559a3e21789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/paul/projects/text_for_app/jean_blog.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/paul/projects/text_for_app/emploi étudia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  number\n",
       "0     /home/paul/projects/text_for_app/jean_blog.txt       0\n",
       "1  /home/paul/projects/text_for_app/emploi étudia...       1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_to_process_df = pd.DataFrame(\n",
    "                        list(\n",
    "                            db.aql.execute('''\n",
    "                                            FOR doc in docs\n",
    "                                            FILTER doc.processed == 'False'\n",
    "                                            RETURN {path :doc.doc_path, number : doc.doc_number}\n",
    "                                            ''')\n",
    "                            )\n",
    "                        )\n",
    "texts_to_process_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026546dd-8c66-4427-a9ac-c4f5acfcaaec",
   "metadata": {},
   "source": [
    "Voir l'empreinte mémoire de l'opération. Découper l'output des textes à traiter pour ne pas tout fournir directement à l'opération ci dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff4777cf-abc3-492d-b1f5-87ee8d14862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = list(nlp.pipe([get_text(path) for path in texts_to_process_df['path']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee3a1af2-f9ac-404b-b860-3d8f34cc9f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_sentences(processed_doc, doc_number):\n",
    "    dict_sentences_to_insert = []\n",
    "    for sentence_number, sentence in enumerate(processed_doc.sents):\n",
    "        if sentence.text != ' ':\n",
    "            dict_sentences_to_insert.append({'_key':f'doc{doc_number}sent{sentence_number}',\n",
    "                                             'content':sentence.text})\n",
    "        else:\n",
    "            pass\n",
    "    db.collection('sentences').import_bulk(dict_sentences_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0a7c42c-c224-49ac-8b25-307526168618",
   "metadata": {},
   "outputs": [],
   "source": [
    "for processed_text, doc_number in zip(processed_docs,texts_to_process_df['number']):\n",
    "    insert_sentences(processed_text,doc_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1dbfb4-6e20-4e19-b6ab-42daca6ef69b",
   "metadata": {},
   "source": [
    "Insertion des tokens et lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bca780-d91b-48bf-b25d-63d08569bc2f",
   "metadata": {},
   "source": [
    "Récupérer le vocabulaire token et lemma de chaque doc et les insérer dans la db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a93447-8583-44dd-aa24-be643f1dc85a",
   "metadata": {},
   "source": [
    "- clé = nombre arbitraire d'insertion\n",
    "- valeur = le token ou lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cc2826-6fc9-4bc4-b35d-bb2a78e05eda",
   "metadata": {},
   "source": [
    "Pour insérer de nouveaux mots par la suite faire une requette sur la table des tokens / lemma et insérer ceux qui ne sont pas déjà présents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e83a71-7889-4bee-a35c-8d170d94c8a0",
   "metadata": {},
   "source": [
    "pour les relations faire une requete par phrase où l'on récupère chaque mot et sa clé associée\n",
    "- relier ensuite les mots entre eux en utilisant les clés et la modalité de cette relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7258a4-a7b8-424b-bc1c-1caed15d051e",
   "metadata": {},
   "source": [
    "Comment relier les phrases aux documents et les mots aux phrases ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6d4cd-cceb-4dc3-8ccf-032e66685e96",
   "metadata": {},
   "source": [
    "### Extraction des correspondances lemmes / tokens depuis le texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c76d472b-aaaa-4087-b1e1-b3a148bbf13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_table_from_text(processed_text):\n",
    "    tokens, lemmas = [], []\n",
    "    for token in processed_text:\n",
    "        if not token.is_punct and not token.is_stop and not token.is_space and not token.is_digit:\n",
    "            tokens.append(token.text.lower())\n",
    "            lemmas.append(token.lemma_.lower())\n",
    "    vocab_table = pd.DataFrame({'token':tokens,\n",
    "                                'lemma':lemmas})\n",
    "    return vocab_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a14c80-f90a-48f5-acfe-917848501460",
   "metadata": {},
   "source": [
    "### Construction de deux tables contenant les occurences uniques des lemmes et tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc915556-a603-44c0-a23f-492dd8e59ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_vocabulary(token_lemma_table,word_type):\n",
    "    return(token_lemma_table[word_type].drop_duplicates().reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff11335-c85b-4560-b893-014494f2ac57",
   "metadata": {},
   "source": [
    "### Extraction du vocabulaire existant depuis la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9b780fb-9719-456f-8f9c-b342d80bdeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_in_db():\n",
    "    vocab_tokens_from_db = pd.DataFrame(\n",
    "        list(db.aql.execute(\"\"\"\n",
    "            FOR doc in tokens\n",
    "            RETURN {word :doc.token, key : doc._key}\n",
    "            \"\"\"))\n",
    "        )\n",
    "        \n",
    "    vocab_lemmas_from_db = pd.DataFrame(\n",
    "        list(db.aql.execute(\"\"\"\n",
    "            FOR doc in lemmas\n",
    "            RETURN {word :doc.lemma, key : doc._key}\n",
    "            \"\"\"))\n",
    "        )\n",
    "    return vocab_tokens_from_db, vocab_lemmas_from_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d72934-838b-40a3-a63a-e78fad879ef3",
   "metadata": {},
   "source": [
    "### Comparaison du vocabulaire extrait du texte avec celui déjà contenu dans la db\n",
    "- On ne rajoute que les instances jamais vues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "930abf54-e511-471a-818a-d7264db99b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_only_new_vocab(vocab_from_text,vocab_from_db):\n",
    "    new_vocab = vocab_from_text[~vocab_from_text.isin(vocab_from_db['word'])].reset_index(drop=True)\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c910ef-1fb6-452c-ad1a-7c577befb09b",
   "metadata": {},
   "source": [
    "A refactoriser :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1b90650-55f2-4215-86bb-04abb602f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vocab_to_db(processed_text):\n",
    "    token_to_lemma_table = get_vocab_table_from_text(processed_text)\n",
    "    \n",
    "    text_tokens_series = unique_vocabulary(token_to_lemma_table,'token')\n",
    "    text_lemmas_series = unique_vocabulary(token_to_lemma_table,'lemma')\n",
    "    \n",
    "    tokens_from_db, lemmas_from_db = get_vocab_in_db()\n",
    "    \n",
    "    if tokens_from_db.shape == (0,0):\n",
    "        dict_list_tokens_to_insert = []\n",
    "        for index, token in zip(text_tokens_series.index,text_tokens_series.values):\n",
    "            dict_list_tokens_to_insert.append({\"_key\":f'token{index}',\n",
    "                                               'token':token})\n",
    "        db.collection('tokens').import_bulk(dict_list_tokens_to_insert)\n",
    "\n",
    "        dict_list_lemmas_to_insert = []\n",
    "        for index, lemma in zip(text_lemmas_series.index,text_lemmas_series.values):\n",
    "            dict_list_lemmas_to_insert.append({\"_key\":f'lemma{index}',\n",
    "                                               'lemma':lemma})\n",
    "        db.collection('lemmas').import_bulk(dict_list_lemmas_to_insert)\n",
    "\n",
    "    else :\n",
    "        new_vocab_tokens = keep_only_new_vocab(text_tokens_series,tokens_from_db)\n",
    "        new_vocab_lemmas = keep_only_new_vocab(text_lemmas_series,lemmas_from_db)\n",
    "\n",
    "        new_vocab_tokens.index = new_vocab_tokens.index + tokens_from_db.shape[0] + 1\n",
    "        new_vocab_lemmas.index = new_vocab_lemmas.index + lemmas_from_db.shape[0] + 1\n",
    "\n",
    "        dict_list_tokens_to_insert = []\n",
    "        for index, token in zip(new_vocab_tokens.index,new_vocab_tokens.values):\n",
    "            dict_list_tokens_to_insert.append({\"_key\":f'token{index}',\n",
    "                                               'token':token})\n",
    "        db.collection('tokens').import_bulk(dict_list_tokens_to_insert)\n",
    "\n",
    "        dict_list_lemmas_to_insert = []\n",
    "        for index, lemma in zip(new_vocab_lemmas.index,new_vocab_lemmas.values):\n",
    "            dict_list_lemmas_to_insert.append({\"_key\":f'lemma{index}',\n",
    "                                               'lemma':lemma})\n",
    "        db.collection('lemmas').import_bulk(dict_list_lemmas_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d276bbe-4209-4eb8-8b77-60feabb1aa1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faire</td>\n",
       "      <td>faire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inutilement</td>\n",
       "      <td>inutilement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>durer</td>\n",
       "      <td>durer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suspense</td>\n",
       "      <td>suspense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>réponse</td>\n",
       "      <td>réponse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>évoluer</td>\n",
       "      <td>évoluer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>cesse</td>\n",
       "      <td>cesse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>jamais</td>\n",
       "      <td>jamais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>étonner</td>\n",
       "      <td>étonner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>intéresse</td>\n",
       "      <td>intéresse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           token        lemma\n",
       "0          faire        faire\n",
       "1    inutilement  inutilement\n",
       "2          durer        durer\n",
       "3       suspense     suspense\n",
       "4        réponse      réponse\n",
       "..           ...          ...\n",
       "106      évoluer      évoluer\n",
       "107        cesse        cesse\n",
       "108       jamais       jamais\n",
       "109      étonner      étonner\n",
       "110    intéresse    intéresse\n",
       "\n",
       "[111 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vocab_table_from_text(processed_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17a5d00-8f56-4402-b3db-50c3e08f1665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed848ba2-3663-4e6c-82dd-a27d95a06317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d28d7260-9e8c-4d6a-ab9c-a606ca9fee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_vocab_to_db(processed_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98fef90c-5a88-465e-9df1-0e1e57480fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_vocab_to_db(processed_docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d7af4c-274a-4919-a6f0-e75cf0cfd109",
   "metadata": {},
   "source": [
    "### Relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df0bcc3-81ca-4f01-831c-5cb82d7fdfa3",
   "metadata": {},
   "source": [
    "Les 3 types de relations dans le graphe, ce qu'elles connectent et ce qu'elles contiennent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71851b-a31c-4225-9668-76d7ac2e711a",
   "metadata": {},
   "source": [
    "phrases aux docs :\n",
    "- faire un call sur les phrases de la db \n",
    "- clé from = la clé de chaque phrase \n",
    "- clé to = la première partie de la clé "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cb76a79-fe26-400f-9305-6048d0bdcd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_keys = pd.Series(list(db.aql.execute('''\n",
    "                        FOR doc in sentences\n",
    "                        return doc._key\n",
    "                        ''')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cca4018b-bf4d-410e-b676-364d9883dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_number_for_sentences = sentences_keys.str.extract('(\\d+)')[0]\n",
    "sentences_number = sentences_keys.str.extract('\\D+\\d+\\D+(\\d+)')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "802c89c6-0961-4bfa-b0e1-595b07c4795b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       doc0sent0\n",
       "1       doc0sent1\n",
       "2      doc0sent11\n",
       "3      doc0sent12\n",
       "4      doc0sent13\n",
       "          ...    \n",
       "531    doc1sent95\n",
       "532    doc1sent96\n",
       "533    doc1sent97\n",
       "534    doc1sent98\n",
       "535    doc1sent99\n",
       "Length: 536, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eb3654-3ab1-42c0-8aab-098ea84b82f3",
   "metadata": {},
   "source": [
    "is_from :\n",
    "- les phrases aux documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0a652de-f45f-4161-874d-266cd216b383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': False,\n",
       " 'created': 536,\n",
       " 'errors': 0,\n",
       " 'empty': 0,\n",
       " 'updated': 0,\n",
       " 'ignored': 0,\n",
       " 'details': []}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_is_from_sent_doc_to_insert = []\n",
    "for sentence_key, doc_number, sentence_number in zip(sentences_keys.values,doc_number_for_sentences,sentences_number.values):\n",
    "    dict_is_from_sent_doc_to_insert.append({'_from':sentence_key,\n",
    "                                            '_to':f'doc{doc_number}',\n",
    "                                            'sentence_number': sentence_number})\n",
    "db.collection('is_from').import_bulk(dict_is_from_sent_doc_to_insert,\n",
    "                                     from_prefix='sentences/',\n",
    "                                     to_prefix='docs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e931051c-e39f-4a20-9b2e-672b9152dd57",
   "metadata": {},
   "source": [
    "is_from :\n",
    "- les tokens aux phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72783cc-9582-40cb-b095-f33d754898e5",
   "metadata": {},
   "source": [
    "Extraction d'une forme tabulaire de la structure syntagmatique.\n",
    "  - Le processus est itéré sur l'ensemble des phrases du document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0bd5ce7a-d30c-41cb-82fc-e79adf7a2a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dependancy_df(processed_text,doc_number):\n",
    "    token_text, token_dep, token_head_text, token_head_pos, sentence_number = [], [], [], [],[]\n",
    "\n",
    "    for count, sentence in enumerate(processed_text.sents):\n",
    "        for token in sentence:\n",
    "            if not token.is_punct and not token.is_stop and not token.is_space and not token.is_digit:\n",
    "                token_text.append(token.text)\n",
    "                token_dep.append(token.dep_), \n",
    "                token_head_text.append(token.head.text), \n",
    "                token_head_pos.append( token.head.pos_)\n",
    "                sentence_number.append(f'doc{doc_number}sent{count}')\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame({'token':token_text,\n",
    "                       'dep':token_dep,\n",
    "                       'head_text':token_head_text,\n",
    "                       'head_pos':token_head_pos,\n",
    "                       'sentence_number':sentence_number})   \n",
    "    df = df[df['head_pos']!=\"SPACE\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "069bb01f-f55d-4888-9fec-f51d09613031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dependancy_df(processed_text,doc_number):\n",
    "    token_text, token_lemma, token_dep, token_head_text, token_head_pos, sentence_number = [], [], [], [], [], []\n",
    "\n",
    "    for count, sentence in enumerate(processed_text.sents):\n",
    "        for token in sentence:\n",
    "            if not token.is_punct and not token.is_stop and not token.is_space and not token.is_digit:\n",
    "                token_text.append(token.text)\n",
    "                token_lemma.append(token.lemma_)\n",
    "                token_dep.append(token.dep_), \n",
    "                token_head_text.append(token.head.text), \n",
    "                token_head_pos.append( token.head.pos_)\n",
    "                sentence_number.append(f'doc{doc_number}sent{count}')\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame({'token':token_text,\n",
    "                       'lemma':token_lemma,\n",
    "                       'dep':token_dep,\n",
    "                       'head_text':token_head_text,\n",
    "                       'head_pos':token_head_pos,\n",
    "                       'sentence_number':sentence_number})   \n",
    "    df = df[df['head_pos']!=\"SPACE\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d0b610dd-85a7-4c19-a3bf-59006d2358f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>dep</th>\n",
       "      <th>head_text</th>\n",
       "      <th>head_pos</th>\n",
       "      <th>sentence_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faire</td>\n",
       "      <td>faire</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>vais</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inutilement</td>\n",
       "      <td>inutilement</td>\n",
       "      <td>advmod</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>durer</td>\n",
       "      <td>durer</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suspense</td>\n",
       "      <td>suspense</td>\n",
       "      <td>obj</td>\n",
       "      <td>durer</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>réponse</td>\n",
       "      <td>réponse</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>oui</td>\n",
       "      <td>ADV</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>évoluer</td>\n",
       "      <td>évoluer</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>peuvent</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>cesse</td>\n",
       "      <td>cesse</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>jamais</td>\n",
       "      <td>jamais</td>\n",
       "      <td>advmod</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>étonner</td>\n",
       "      <td>étonner</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>intéresse</td>\n",
       "      <td>intéresse</td>\n",
       "      <td>conj</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           token        lemma     dep head_text head_pos sentence_number\n",
       "0          faire        faire   xcomp      vais     VERB       doc0sent0\n",
       "1    inutilement  inutilement  advmod     faire     VERB       doc0sent0\n",
       "2          durer        durer   xcomp     faire     VERB       doc0sent0\n",
       "3       suspense     suspense     obj     durer     VERB       doc0sent0\n",
       "4        réponse      réponse   nsubj       oui      ADV       doc0sent0\n",
       "..           ...          ...     ...       ...      ...             ...\n",
       "106      évoluer      évoluer   xcomp   peuvent     VERB      doc0sent15\n",
       "107        cesse        cesse    ROOT     cesse     VERB      doc0sent16\n",
       "108       jamais       jamais  advmod     cesse     VERB      doc0sent16\n",
       "109      étonner      étonner   xcomp     cesse     VERB      doc0sent16\n",
       "110    intéresse    intéresse    conj     cesse     VERB      doc0sent16\n",
       "\n",
       "[111 rows x 6 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependancy_df = create_dependancy_df(processed_docs[0],0) \n",
    "dependancy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26d92d50-945c-4e5f-806e-d55f4ffbd604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faire</td>\n",
       "      <td>token0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inutilement</td>\n",
       "      <td>token1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>durer</td>\n",
       "      <td>token2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suspense</td>\n",
       "      <td>token3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>réponse</td>\n",
       "      <td>token4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2855</th>\n",
       "      <td>florence</td>\n",
       "      <td>token2856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>lefresne</td>\n",
       "      <td>token2857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2857</th>\n",
       "      <td>vecteurs</td>\n",
       "      <td>token2858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2858</th>\n",
       "      <td>structurelle</td>\n",
       "      <td>token2859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2859</th>\n",
       "      <td>juillet</td>\n",
       "      <td>token2860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2860 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word        key\n",
       "0            faire     token0\n",
       "1      inutilement     token1\n",
       "2            durer     token2\n",
       "3         suspense     token3\n",
       "4          réponse     token4\n",
       "...            ...        ...\n",
       "2855      florence  token2856\n",
       "2856      lefresne  token2857\n",
       "2857      vecteurs  token2858\n",
       "2858  structurelle  token2859\n",
       "2859       juillet  token2860\n",
       "\n",
       "[2860 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_table_tokens_in_db = get_vocab_in_db()[0]\n",
    "vocab_table_tokens_in_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cb4c00d5-cda3-4390-b118-69a7030a52ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_from_sentence_table = dependancy_df.merge(vocab_table_tokens_in_db, \n",
    "                                                        left_on='token', \n",
    "                                                        right_on='word', sort = 'outer')\\\n",
    "                                                        .rename(columns={'key':'token_key'})\\\n",
    "                                                        .drop('word',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d3760dba-26c3-486d-bade-972e3a4352b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>dep</th>\n",
       "      <th>head_text</th>\n",
       "      <th>head_pos</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>token_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aberration</td>\n",
       "      <td>aberration</td>\n",
       "      <td>conj</td>\n",
       "      <td>est</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent7</td>\n",
       "      <td>token33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aborder</td>\n",
       "      <td>aborder</td>\n",
       "      <td>acl</td>\n",
       "      <td>manière</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>doc0sent15</td>\n",
       "      <td>token85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aime</td>\n",
       "      <td>aime</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>aime</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent7</td>\n",
       "      <td>token35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air</td>\n",
       "      <td>air</td>\n",
       "      <td>obj</td>\n",
       "      <td>donne</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent1</td>\n",
       "      <td>token13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ans</td>\n",
       "      <td>an</td>\n",
       "      <td>obl:mod</td>\n",
       "      <td>commence</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent12</td>\n",
       "      <td>token61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>vêtement</td>\n",
       "      <td>vêtement</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>vêtement</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>doc0sent12</td>\n",
       "      <td>token45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>vêtement</td>\n",
       "      <td>vêtement</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>peut</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent12</td>\n",
       "      <td>token45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>vêtement</td>\n",
       "      <td>vêtement</td>\n",
       "      <td>obj</td>\n",
       "      <td>aborder</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent15</td>\n",
       "      <td>token45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>étonner</td>\n",
       "      <td>étonner</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "      <td>token90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>évoluer</td>\n",
       "      <td>évoluer</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>peuvent</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent15</td>\n",
       "      <td>token87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          token       lemma      dep head_text head_pos sentence_number  \\\n",
       "0    aberration  aberration     conj       est     VERB       doc0sent7   \n",
       "1       aborder     aborder      acl   manière     NOUN      doc0sent15   \n",
       "2          aime        aime     ROOT      aime     VERB       doc0sent7   \n",
       "3           air         air      obj     donne     VERB       doc0sent1   \n",
       "4           ans          an  obl:mod  commence     VERB      doc0sent12   \n",
       "..          ...         ...      ...       ...      ...             ...   \n",
       "106    vêtement    vêtement     ROOT  vêtement     NOUN      doc0sent12   \n",
       "107    vêtement    vêtement    nsubj      peut     VERB      doc0sent12   \n",
       "108    vêtement    vêtement      obj   aborder     VERB      doc0sent15   \n",
       "109     étonner     étonner    xcomp     cesse     VERB      doc0sent16   \n",
       "110     évoluer     évoluer    xcomp   peuvent     VERB      doc0sent15   \n",
       "\n",
       "    token_key  \n",
       "0     token33  \n",
       "1     token85  \n",
       "2     token35  \n",
       "3     token13  \n",
       "4     token61  \n",
       "..        ...  \n",
       "106   token45  \n",
       "107   token45  \n",
       "108   token45  \n",
       "109   token90  \n",
       "110   token87  \n",
       "\n",
       "[111 rows x 7 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_from_sentence_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "11c5251e-4b85-432d-8aa9-d1eb722e344c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>dep</th>\n",
       "      <th>head_text</th>\n",
       "      <th>head_pos</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>token_key</th>\n",
       "      <th>head_text_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>perverse</td>\n",
       "      <td>pervers</td>\n",
       "      <td>amod</td>\n",
       "      <td>aberration</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>doc0sent7</td>\n",
       "      <td>token34</td>\n",
       "      <td>token33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vêtement</td>\n",
       "      <td>vêtement</td>\n",
       "      <td>obj</td>\n",
       "      <td>aborder</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent15</td>\n",
       "      <td>token45</td>\n",
       "      <td>token85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aime</td>\n",
       "      <td>aime</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>aime</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent7</td>\n",
       "      <td>token35</td>\n",
       "      <td>token35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beaucoup</td>\n",
       "      <td>beaucoup</td>\n",
       "      <td>advmod</td>\n",
       "      <td>aime</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent7</td>\n",
       "      <td>token36</td>\n",
       "      <td>token35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>porter</td>\n",
       "      <td>porter</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>aime</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent7</td>\n",
       "      <td>token37</td>\n",
       "      <td>token35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>valeur</td>\n",
       "      <td>valeur</td>\n",
       "      <td>obl:mod</td>\n",
       "      <td>trouve</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent13</td>\n",
       "      <td>token49</td>\n",
       "      <td>token67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>pareil</td>\n",
       "      <td>pareil</td>\n",
       "      <td>amod</td>\n",
       "      <td>truc</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>doc0sent3</td>\n",
       "      <td>token24</td>\n",
       "      <td>token23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>plan</td>\n",
       "      <td>plan</td>\n",
       "      <td>nmod</td>\n",
       "      <td>valeur</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>doc0sent13</td>\n",
       "      <td>token70</td>\n",
       "      <td>token49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>symbolique</td>\n",
       "      <td>symbolique</td>\n",
       "      <td>conj</td>\n",
       "      <td>vêtement</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>doc0sent15</td>\n",
       "      <td>token86</td>\n",
       "      <td>token45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>vêtement</td>\n",
       "      <td>vêtement</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>vêtement</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>doc0sent12</td>\n",
       "      <td>token45</td>\n",
       "      <td>token45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         token       lemma      dep   head_text head_pos sentence_number  \\\n",
       "0     perverse     pervers     amod  aberration     NOUN       doc0sent7   \n",
       "1     vêtement    vêtement      obj     aborder     VERB      doc0sent15   \n",
       "2         aime        aime     ROOT        aime     VERB       doc0sent7   \n",
       "3     beaucoup    beaucoup   advmod        aime     VERB       doc0sent7   \n",
       "4       porter      porter    xcomp        aime     VERB       doc0sent7   \n",
       "..         ...         ...      ...         ...      ...             ...   \n",
       "77      valeur      valeur  obl:mod      trouve     VERB      doc0sent13   \n",
       "78      pareil      pareil     amod        truc     NOUN       doc0sent3   \n",
       "79        plan        plan     nmod      valeur     NOUN      doc0sent13   \n",
       "80  symbolique  symbolique     conj    vêtement     NOUN      doc0sent15   \n",
       "81    vêtement    vêtement     ROOT    vêtement     NOUN      doc0sent12   \n",
       "\n",
       "   token_key head_text_key  \n",
       "0    token34       token33  \n",
       "1    token45       token85  \n",
       "2    token35       token35  \n",
       "3    token36       token35  \n",
       "4    token37       token35  \n",
       "..       ...           ...  \n",
       "77   token49       token67  \n",
       "78   token24       token23  \n",
       "79   token70       token49  \n",
       "80   token86       token45  \n",
       "81   token45       token45  \n",
       "\n",
       "[82 rows x 8 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependancy_table_for_insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "93298775-cc20-4e9c-9ac9-10f53ceffc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_from_sentence_table = dependancy_df.merge(vocab_table_tokens_in_db, \n",
    "                                                        left_on='token', \n",
    "                                                        right_on='word')\\\n",
    "                                                        .rename(columns={'key':'token_key'})\\\n",
    "                                                        .drop('word',axis=1)\n",
    "\n",
    "dependancy_table_for_insert = token_from_sentence_table.merge(vocab_table_tokens_in_db,\n",
    "                                                        left_on='head_text',\n",
    "                                                        right_on='word')\\\n",
    "                                                        .rename(columns={'key':'head_text_key'})\\\n",
    "                                                        .drop('word',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4b86fe4-7547-4b5f-8107-8cb64c0fba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_is_from_sent_token_to_insert = []\n",
    "for token_key,sentence_number in zip(token_from_sentence_table['token_key'],token_from_sentence_table['sentence_number'] ):\n",
    "    dict_is_from_sent_token_to_insert.append({'_from':token_key,\n",
    "                                            '_to':sentence_number})\n",
    "db.collection('is_from').import_bulk(dict_is_from_sent_token_to_insert,\n",
    "                                     from_prefix='tokens/',\n",
    "                                     to_prefix='sentences/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17299a75-99a9-4815-85a3-c371481f08fc",
   "metadata": {},
   "source": [
    "syntagmatic_link :\n",
    "- les tokens aux tokens \n",
    "    - le type de relation grammaticale\n",
    "    - l'ID de la phrase contenant cette relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b1222-3b8b-417c-b2d2-fe3821c30601",
   "metadata": {},
   "source": [
    "Merge des tokens avec les id correspondants dans la db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b8ed3-894e-4c24-b9ce-63c2b2b129a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faire</td>\n",
       "      <td>token0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inutilement</td>\n",
       "      <td>token1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>durer</td>\n",
       "      <td>token2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suspense</td>\n",
       "      <td>token3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>réponse</td>\n",
       "      <td>token4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2855</th>\n",
       "      <td>florence</td>\n",
       "      <td>token2856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>lefresne</td>\n",
       "      <td>token2857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2857</th>\n",
       "      <td>vecteurs</td>\n",
       "      <td>token2858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2858</th>\n",
       "      <td>structurelle</td>\n",
       "      <td>token2859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2859</th>\n",
       "      <td>juillet</td>\n",
       "      <td>token2860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2860 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word        key\n",
       "0            faire     token0\n",
       "1      inutilement     token1\n",
       "2            durer     token2\n",
       "3         suspense     token3\n",
       "4          réponse     token4\n",
       "...            ...        ...\n",
       "2855      florence  token2856\n",
       "2856      lefresne  token2857\n",
       "2857      vecteurs  token2858\n",
       "2858  structurelle  token2859\n",
       "2859       juillet  token2860\n",
       "\n",
       "[2860 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vocab_in_db()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f319462e-aac1-4ab9-b112-3525dedc644c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>dep</th>\n",
       "      <th>head_text</th>\n",
       "      <th>head_pos</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>token_key</th>\n",
       "      <th>head_text_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faire</td>\n",
       "      <td>faire</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>vais</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "      <td>token0</td>\n",
       "      <td>token0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faire</td>\n",
       "      <td>faire</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent9</td>\n",
       "      <td>token0</td>\n",
       "      <td>token0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inutilement</td>\n",
       "      <td>inutilement</td>\n",
       "      <td>advmod</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "      <td>token1</td>\n",
       "      <td>token1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>durer</td>\n",
       "      <td>durer</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "      <td>token2</td>\n",
       "      <td>token2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suspense</td>\n",
       "      <td>suspense</td>\n",
       "      <td>obj</td>\n",
       "      <td>durer</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "      <td>token3</td>\n",
       "      <td>token3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>évoluer</td>\n",
       "      <td>évoluer</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>peuvent</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent15</td>\n",
       "      <td>token87</td>\n",
       "      <td>token87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>cesse</td>\n",
       "      <td>cesse</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "      <td>token88</td>\n",
       "      <td>token88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>jamais</td>\n",
       "      <td>jamais</td>\n",
       "      <td>advmod</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "      <td>token89</td>\n",
       "      <td>token89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>étonner</td>\n",
       "      <td>étonner</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "      <td>token90</td>\n",
       "      <td>token90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>intéresse</td>\n",
       "      <td>intéresse</td>\n",
       "      <td>conj</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "      <td>token91</td>\n",
       "      <td>token91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           token        lemma     dep head_text head_pos sentence_number  \\\n",
       "0          faire        faire   xcomp      vais     VERB       doc0sent0   \n",
       "1          faire        faire    ROOT     faire     VERB       doc0sent9   \n",
       "2    inutilement  inutilement  advmod     faire     VERB       doc0sent0   \n",
       "3          durer        durer   xcomp     faire     VERB       doc0sent0   \n",
       "4       suspense     suspense     obj     durer     VERB       doc0sent0   \n",
       "..           ...          ...     ...       ...      ...             ...   \n",
       "106      évoluer      évoluer   xcomp   peuvent     VERB      doc0sent15   \n",
       "107        cesse        cesse    ROOT     cesse     VERB      doc0sent16   \n",
       "108       jamais       jamais  advmod     cesse     VERB      doc0sent16   \n",
       "109      étonner      étonner   xcomp     cesse     VERB      doc0sent16   \n",
       "110    intéresse    intéresse    conj     cesse     VERB      doc0sent16   \n",
       "\n",
       "    token_key head_text_key  \n",
       "0      token0        token0  \n",
       "1      token0        token0  \n",
       "2      token1        token1  \n",
       "3      token2        token2  \n",
       "4      token3        token3  \n",
       "..        ...           ...  \n",
       "106   token87       token87  \n",
       "107   token88       token88  \n",
       "108   token89       token89  \n",
       "109   token90       token90  \n",
       "110   token91       token91  \n",
       "\n",
       "[111 rows x 8 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependancy_table_for_insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd450742-8fb0-4608-9f00-51d12f615349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': False,\n",
       " 'created': 82,\n",
       " 'errors': 0,\n",
       " 'empty': 0,\n",
       " 'updated': 0,\n",
       " 'ignored': 0,\n",
       " 'details': []}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_syntagmatic_link_to_insert = []\n",
    "for head_text_key, token_key, dep_relation, head_pos_tag, sentence_number in zip(dependancy_table_for_insert['head_text_key'],\n",
    "                                                                                 dependancy_table_for_insert['token_key'],\n",
    "                                                                                 dependancy_table_for_insert['dep'],\n",
    "                                                                                 dependancy_table_for_insert['head_pos'],\n",
    "                                                                                 dependancy_table_for_insert['sentence_number']):\n",
    "    dict_syntagmatic_link_to_insert.append({'_from':head_text_key,\n",
    "                                            '_to':token_key,\n",
    "                                            'dep_relation':dep_relation,\n",
    "                                            'head_pos_tag':head_pos_tag,\n",
    "                                            'from_sentence_number':sentence_number})\n",
    "db.collection('syntagmatic_link').import_bulk(dict_syntagmatic_link_to_insert,\n",
    "                                             from_prefix='tokens/',\n",
    "                                             to_prefix='tokens/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93900680-0645-474c-9a78-13fc34cf7166",
   "metadata": {},
   "source": [
    "contracts_to :\n",
    "- Les tokens aux lemmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4436231-35c4-4777-818e-d6966c22ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_in_db = get_vocab_in_db()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5a8a9a5c-f108-493c-b1fc-2323e2acd15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_to_table = dependancy_table_for_insert.merge(lemmas_in_db, left_on='lemma', right_on='word')\\\n",
    "                        .rename(columns={'key':'lemma_key'})\\\n",
    "                        .drop('word',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6d4a7295-366d-4dc0-89e5-b9a042d18de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>dep</th>\n",
       "      <th>head_text</th>\n",
       "      <th>head_pos</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>token_key</th>\n",
       "      <th>head_text_key</th>\n",
       "      <th>lemma_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>perverse</td>\n",
       "      <td>pervers</td>\n",
       "      <td>amod</td>\n",
       "      <td>aberration</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>doc0sent7</td>\n",
       "      <td>token34</td>\n",
       "      <td>token33</td>\n",
       "      <td>lemma34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vêtement</td>\n",
       "      <td>vêtement</td>\n",
       "      <td>obj</td>\n",
       "      <td>aborder</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent15</td>\n",
       "      <td>token45</td>\n",
       "      <td>token85</td>\n",
       "      <td>lemma45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vêtement</td>\n",
       "      <td>vêtement</td>\n",
       "      <td>obj</td>\n",
       "      <td>porte</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent11</td>\n",
       "      <td>token45</td>\n",
       "      <td>token44</td>\n",
       "      <td>lemma45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vêtement</td>\n",
       "      <td>vêtement</td>\n",
       "      <td>nmod</td>\n",
       "      <td>style</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>doc0sent11</td>\n",
       "      <td>token45</td>\n",
       "      <td>token46</td>\n",
       "      <td>lemma45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vêtement</td>\n",
       "      <td>vêtement</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>vêtement</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>doc0sent12</td>\n",
       "      <td>token45</td>\n",
       "      <td>token45</td>\n",
       "      <td>lemma45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>principal</td>\n",
       "      <td>principal</td>\n",
       "      <td>amod</td>\n",
       "      <td>sujet</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>doc0sent12</td>\n",
       "      <td>token54</td>\n",
       "      <td>token53</td>\n",
       "      <td>lemma54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>trouve</td>\n",
       "      <td>trouve</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>trouve</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent13</td>\n",
       "      <td>token67</td>\n",
       "      <td>token67</td>\n",
       "      <td>lemma67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>pareil</td>\n",
       "      <td>pareil</td>\n",
       "      <td>amod</td>\n",
       "      <td>truc</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>doc0sent3</td>\n",
       "      <td>token24</td>\n",
       "      <td>token23</td>\n",
       "      <td>lemma24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>plan</td>\n",
       "      <td>plan</td>\n",
       "      <td>nmod</td>\n",
       "      <td>valeur</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>doc0sent13</td>\n",
       "      <td>token70</td>\n",
       "      <td>token49</td>\n",
       "      <td>lemma70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>symbolique</td>\n",
       "      <td>symbolique</td>\n",
       "      <td>conj</td>\n",
       "      <td>vêtement</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>doc0sent15</td>\n",
       "      <td>token86</td>\n",
       "      <td>token45</td>\n",
       "      <td>lemma86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         token       lemma   dep   head_text head_pos sentence_number  \\\n",
       "0     perverse     pervers  amod  aberration     NOUN       doc0sent7   \n",
       "1     vêtement    vêtement   obj     aborder     VERB      doc0sent15   \n",
       "2     vêtement    vêtement   obj       porte     VERB      doc0sent11   \n",
       "3     vêtement    vêtement  nmod       style     NOUN      doc0sent11   \n",
       "4     vêtement    vêtement  ROOT    vêtement     NOUN      doc0sent12   \n",
       "..         ...         ...   ...         ...      ...             ...   \n",
       "77   principal   principal  amod       sujet     NOUN      doc0sent12   \n",
       "78      trouve      trouve  ROOT      trouve     VERB      doc0sent13   \n",
       "79      pareil      pareil  amod        truc     NOUN       doc0sent3   \n",
       "80        plan        plan  nmod      valeur     NOUN      doc0sent13   \n",
       "81  symbolique  symbolique  conj    vêtement     NOUN      doc0sent15   \n",
       "\n",
       "   token_key head_text_key lemma_key  \n",
       "0    token34       token33   lemma34  \n",
       "1    token45       token85   lemma45  \n",
       "2    token45       token44   lemma45  \n",
       "3    token45       token46   lemma45  \n",
       "4    token45       token45   lemma45  \n",
       "..       ...           ...       ...  \n",
       "77   token54       token53   lemma54  \n",
       "78   token67       token67   lemma67  \n",
       "79   token24       token23   lemma24  \n",
       "80   token70       token49   lemma70  \n",
       "81   token86       token45   lemma86  \n",
       "\n",
       "[82 rows x 9 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contracts_to_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4ba4fa4a-b5a1-4342-b56d-a304c8734861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': False,\n",
       " 'created': 82,\n",
       " 'errors': 0,\n",
       " 'empty': 0,\n",
       " 'updated': 0,\n",
       " 'ignored': 0,\n",
       " 'details': []}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_contracts_to = []\n",
    "for token_key, lemma_key, sentence_number in zip(contracts_to_table['token_key'],\n",
    "                                                 contracts_to_table['lemma_key'],\n",
    "                                                 contracts_to_table['sentence_number']):\n",
    "    \n",
    "    dict_contracts_to.append({'_from':token_key,\n",
    "                             '_to':lemma_key,\n",
    "                             'sentence_number':sentence_number})\n",
    "    \n",
    "db.collection('contracts_to').import_bulk(dict_contracts_to,\n",
    "                                             from_prefix='tokens/',\n",
    "                                             to_prefix='lemmas/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8533de3-3f27-489e-af73-43e3593715f8",
   "metadata": {},
   "source": [
    "# TODO :\n",
    "- Insérer une logique de check database / non répétition des insertions lors de l'insertion de nouvelles phrases\n",
    "- Assigner dans is_from entre les mots et les phrases la clé de la phrase (doc0sent20) -check\n",
    "- connecter les lemmes avec les tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4bbff409-4143-4ddf-9c63-4ae200feee0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_from</th>\n",
       "      <th>_to</th>\n",
       "      <th>dep_relation</th>\n",
       "      <th>head_pos_tag</th>\n",
       "      <th>from_sentence_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>token0</td>\n",
       "      <td>token0</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>token0</td>\n",
       "      <td>token1</td>\n",
       "      <td>advmod</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>token0</td>\n",
       "      <td>token2</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>token0</td>\n",
       "      <td>token38</td>\n",
       "      <td>advmod</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>token0</td>\n",
       "      <td>token39</td>\n",
       "      <td>obj</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>token84</td>\n",
       "      <td>token85</td>\n",
       "      <td>acl</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>doc0sent15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>token88</td>\n",
       "      <td>token88</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>token88</td>\n",
       "      <td>token89</td>\n",
       "      <td>advmod</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>token88</td>\n",
       "      <td>token90</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>token88</td>\n",
       "      <td>token91</td>\n",
       "      <td>conj</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      _from      _to dep_relation head_pos_tag from_sentence_number\n",
       "0    token0   token0         ROOT         VERB            doc0sent9\n",
       "1    token0   token1       advmod         VERB            doc0sent0\n",
       "2    token0   token2        xcomp         VERB            doc0sent0\n",
       "3    token0  token38       advmod         VERB            doc0sent9\n",
       "4    token0  token39          obj         VERB            doc0sent9\n",
       "..      ...      ...          ...          ...                  ...\n",
       "77  token84  token85          acl         NOUN           doc0sent15\n",
       "78  token88  token88         ROOT         VERB           doc0sent16\n",
       "79  token88  token89       advmod         VERB           doc0sent16\n",
       "80  token88  token90        xcomp         VERB           doc0sent16\n",
       "81  token88  token91         conj         VERB           doc0sent16\n",
       "\n",
       "[82 rows x 5 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dict_syntagmatic_link_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147b318e-bfde-407f-b406-17afbabe2c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33fb85d-2c34-4474-8197-1917412906ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7f9411-7ec6-44f3-87d8-8f1dd14755f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6714ae7e-fccc-49d0-84dc-d0e5bf9319ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da43c033-cdaf-4fc0-915b-c8d92393bb39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eb2449-b5f1-4ebf-838b-b8df5778af8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a108809c-d4ec-41b0-8295-86c245b5d7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced1d7ac-a9a3-416e-a066-5a242892a170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
