{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc6c7873-5841-4927-ad07-c4d9c9643b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import pyArango\n",
    "import os\n",
    "from os import path\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "from arango import ArangoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102ea8a7-7052-4641-af64-9fea04316dff",
   "metadata": {},
   "source": [
    "Initialisation de la connection avec la base de données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7232caf-3eff-4b8b-bd0e-56ba3bc44a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the client for ArangoDB.\n",
    "client = ArangoClient(hosts=\"http://localhost:8529\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cb3f359-7e8e-49bf-87a4-9fd3b76a11ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_db = client.db('_system', username='root',password='root')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987f104-3e32-4c55-9e8c-6ea4f499644e",
   "metadata": {},
   "source": [
    "si besoin de repartir à zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "65c6106a-66bb-47dd-a771-5881e9e203c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_db.delete_database('text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5bb57b-0e9e-411b-8cc1-502108436582",
   "metadata": {},
   "source": [
    "Check si la base de données existe déjà et création si besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a20a9e06-ed07-4f5b-8b47-00d6041d6040",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not sys_db.has_database('text'):\n",
    "    \n",
    "    sys_db.create_database('text')\n",
    "    db = client.db(\"text\", username=\"root\", password=\"root\")\n",
    "    graph = db.create_graph('text_explorer')\n",
    "    # création des collections\n",
    "    docs = graph.create_vertex_collection('docs')\n",
    "    sentences = graph.create_vertex_collection('sentences')\n",
    "    tokens = graph.create_vertex_collection('tokens')\n",
    "    lemmas = graph.create_vertex_collection('lemmas')\n",
    "    # création des arrêtes\n",
    "    is_from = graph.create_edge_definition(\n",
    "        edge_collection='is_from',\n",
    "        from_vertex_collections=['sentences','tokens'],\n",
    "        to_vertex_collections=['docs','sentences']\n",
    "    )\n",
    "    contracts_to = graph.create_edge_definition(\n",
    "        edge_collection='contracts_to',\n",
    "        from_vertex_collections=['tokens'],\n",
    "        to_vertex_collections=['lemmas']\n",
    "    )\n",
    "    syntagmatic_link = graph.create_edge_definition(\n",
    "        edge_collection='syntagmatic_link',\n",
    "        from_vertex_collections=['tokens'],\n",
    "        to_vertex_collections=['tokens']\n",
    "    )\n",
    "else:\n",
    "    db = client.db(\"text\", username=\"root\", password=\"root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57df3b3-4e03-4e6b-9347-a372fd5f0638",
   "metadata": {},
   "source": [
    "L'objectif maintenant est de remplir ces champs avec les données extraites depuis le texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e35a3-c60c-44eb-ab33-aef3f046f55b",
   "metadata": {},
   "source": [
    "Récupération des documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "551cdde1-7d69-4e98-9c99-193778339df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(path):\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        return(f.read().replace('\\n',' '))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e859a1b1-d3d1-4064-b33e-30bf440d5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir_path = os.getcwd()\n",
    "#dir_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d540b4d-726e-4254-bf5a-719c699d4251",
   "metadata": {},
   "source": [
    "Liste des fichiers présents dans le dossier choisi pour l'analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "405e5599-89c9-414b-8e90-8aa71d951494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/paul/projects/text_for_app/jean_blog.txt',\n",
       " '/home/paul/projects/text_for_app/emploi étudiant et inégalités sociales.txt']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('/home/paul/projects/text_for_app/*.{}'.format('txt'))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e6ef6cb5-0757-4017-a3b8-d14be62a678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_from_path(path):\n",
    "    return os.path.normpath(path).split(os.sep)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "490f3921-3654-4653-a4c4-3b59bee566a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>doc_name</th>\n",
       "      <th>doc_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/paul/projects/text_for_app/jean_blog.txt</td>\n",
       "      <td>jean_blog.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/paul/projects/text_for_app/emploi étudia...</td>\n",
       "      <td>emploi étudiant et inégalités sociales.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath  \\\n",
       "0     /home/paul/projects/text_for_app/jean_blog.txt   \n",
       "1  /home/paul/projects/text_for_app/emploi étudia...   \n",
       "\n",
       "                                     doc_name  doc_number  \n",
       "0                               jean_blog.txt           0  \n",
       "1  emploi étudiant et inégalités sociales.txt           1  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = pd.DataFrame({'filepath':files,\n",
    "                          'doc_name':[get_filename_from_path(filepath) for filepath in files],\n",
    "                          'doc_number':list(range(0,len(files)))})\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "11549c59-b792-4492-b26b-be12ac62163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_docs(documents_found):\n",
    "    in_db_doclist = pd.DataFrame(list(db.aql.execute('''FOR doc in docs RETURN doc''')))\n",
    "    if in_db_doclist.shape == (0,0):\n",
    "        pass\n",
    "    else :\n",
    "        documents_found = documents_found[~documents_found['doc_name'].isin(in_db_doclist['doc_name'])]\n",
    "        last_doc_number = in_db_doclist['doc_number'].max()\n",
    "        documents_found['doc_number'] = documents_found['doc_number'] + last_doc_number + 1\n",
    "    \n",
    "    dict_list_documents_to_insert = []\n",
    "    for doc_name, number, path  in zip(documents_found['doc_name'], documents_found['doc_number'], documents_found['filepath']):\n",
    "        dict_list_documents_to_insert.append({'_key':f'doc{number}',\n",
    "                                                'doc_name':doc_name,\n",
    "                                                'doc_path':path,\n",
    "                                                'doc_number':number,\n",
    "                                                'processed':'False'})\n",
    "    db.collection('docs').import_bulk(dict_list_documents_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "72c053c1-169a-443b-a1c3-1d55fdf44a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_docs(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b5745-ffc1-475f-a3f7-b8bf2e4efffe",
   "metadata": {},
   "source": [
    "# Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "445a2ee3-c54b-4013-9c62-47687b08ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dfbdeb-8e90-4c45-bf06-3855041d95eb",
   "metadata": {},
   "source": [
    "Traitement des fichiers par le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4464a67-236a-497a-ae38-c562907e975d",
   "metadata": {},
   "source": [
    "## se renseigner sur comment faire du traitement en batch\n",
    "    - mesurer l'empreinte mémoire de l'opération et ajuster combien de documents en même temps peuvent être traités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f4986fa9-3661-4abb-bfe8-9559a3e21789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/paul/projects/text_for_app/jean_blog.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/paul/projects/text_for_app/emploi étudia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  number\n",
       "0     /home/paul/projects/text_for_app/jean_blog.txt       0\n",
       "1  /home/paul/projects/text_for_app/emploi étudia...       1"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_to_process_df = pd.DataFrame(\n",
    "                        list(\n",
    "                            db.aql.execute('''\n",
    "                                            FOR doc in docs\n",
    "                                            FILTER doc.processed == 'False'\n",
    "                                            RETURN {path :doc.doc_path, number : doc.doc_number}\n",
    "                                            ''')\n",
    "                            )\n",
    "                        )\n",
    "texts_to_process_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026546dd-8c66-4427-a9ac-c4f5acfcaaec",
   "metadata": {},
   "source": [
    "Voir l'empreinte mémoire de l'opération. Découper l'output des textes à traiter pour ne pas tout fournir direction à l'opération ci dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ff4777cf-abc3-492d-b1f5-87ee8d14862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = list(nlp.pipe([get_text(path) for path in texts_to_process_df['path']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ee3a1af2-f9ac-404b-b860-3d8f34cc9f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_sentences(processed_doc, doc_number):\n",
    "    dict_sentences_to_insert = []\n",
    "    for sentence_number, sentence in enumerate(processed_doc.sents):\n",
    "        if sentence.text != ' ':\n",
    "            dict_sentences_to_insert.append({'_key':f'doc{doc_number}sent{sentence_number}',\n",
    "                                             'content':sentence.text})\n",
    "        else:\n",
    "            pass\n",
    "    db.collection('sentences').import_bulk(dict_sentences_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a0a7c42c-c224-49ac-8b25-307526168618",
   "metadata": {},
   "outputs": [],
   "source": [
    "for processed_text, doc_number in zip(processed_docs,texts_to_process_df['number']):\n",
    "    insert_sentences(processed_text,doc_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1dbfb4-6e20-4e19-b6ab-42daca6ef69b",
   "metadata": {},
   "source": [
    "Insertion des tokens et lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bca780-d91b-48bf-b25d-63d08569bc2f",
   "metadata": {},
   "source": [
    "Récupérer le vocabulaire token et lemma de chaque doc et les insérer dans la db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a93447-8583-44dd-aa24-be643f1dc85a",
   "metadata": {},
   "source": [
    "- clé = nombre arbitraire d'insertion\n",
    "- valeur = le token ou lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cc2826-6fc9-4bc4-b35d-bb2a78e05eda",
   "metadata": {},
   "source": [
    "Pour insérer de nouveaux mots par la suite faire une requette sur la table des tokens / lemma et insérer ceux qui ne sont pas déjà présents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e83a71-7889-4bee-a35c-8d170d94c8a0",
   "metadata": {},
   "source": [
    "pour les relations faire une requete par phrase où l'on récupère chaque mot et sa clé associée\n",
    "- relier ensuite les mots entre eux en utilisant les clés et la modalité de cette relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7258a4-a7b8-424b-bc1c-1caed15d051e",
   "metadata": {},
   "source": [
    "Comment relier les phrases aux documents et les mots aux phrases ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6d4cd-cceb-4dc3-8ccf-032e66685e96",
   "metadata": {},
   "source": [
    "### Extraction des correspondances lemmes / tokens depuis le texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d797687f-5d8b-4104-9809-e09303ba3603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_table_from_text(processed_text):\n",
    "    tokens, lemmas = [], []\n",
    "    for token in processed_text:\n",
    "        if not token.is_punct and not token.is_stop and not token.is_space and not token.is_digit:\n",
    "            tokens.append(token.text.lower())\n",
    "            lemmas.append(token.lemma_.lower())\n",
    "    vocab_table = pd.DataFrame({'token':tokens,\n",
    "                                'lemma':lemmas})\n",
    "    return vocab_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a14c80-f90a-48f5-acfe-917848501460",
   "metadata": {},
   "source": [
    "### Construction de deux tables contenant les occurences uniques des lemmes et tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dc915556-a603-44c0-a23f-492dd8e59ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_vocabulary(token_lemma_table,word_type):\n",
    "    return(token_lemma_table[word_type].drop_duplicates().reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff11335-c85b-4560-b893-014494f2ac57",
   "metadata": {},
   "source": [
    "### Extraction du vocabulaire existant depuis la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c9b780fb-9719-456f-8f9c-b342d80bdeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_in_db():\n",
    "    vocab_tokens_from_db = pd.DataFrame(\n",
    "        list(db.aql.execute(\"\"\"\n",
    "            FOR doc in tokens\n",
    "            RETURN {word :doc.token, key : doc._key}\n",
    "            \"\"\"))\n",
    "        )\n",
    "        \n",
    "    vocab_lemmas_from_db = pd.DataFrame(\n",
    "        list(db.aql.execute(\"\"\"\n",
    "            FOR doc in lemmas\n",
    "            RETURN {word :doc.lemma, key : doc._key}\n",
    "            \"\"\"))\n",
    "        )\n",
    "    return vocab_tokens_from_db, vocab_lemmas_from_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d72934-838b-40a3-a63a-e78fad879ef3",
   "metadata": {},
   "source": [
    "### Comparaison du vocabulaire extrait du texte avec celui déjà contenu dans la db\n",
    "- On ne rajoute que les instances jamais vues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "930abf54-e511-471a-818a-d7264db99b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_only_new_vocab(vocab_from_text,vocab_from_db):\n",
    "    new_vocab = vocab_from_text[~vocab_from_text.isin(vocab_from_db['word'])].reset_index(drop=True)\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c910ef-1fb6-452c-ad1a-7c577befb09b",
   "metadata": {},
   "source": [
    "A refactoriser :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b1b90650-55f2-4215-86bb-04abb602f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vocab_to_db(processed_text):\n",
    "    token_to_lemma_table = get_vocab_table_from_text(processed_text)\n",
    "    \n",
    "    text_tokens_series = unique_vocabulary(token_to_lemma_table,'token')\n",
    "    text_lemmas_series = unique_vocabulary(token_to_lemma_table,'lemma')\n",
    "    \n",
    "    tokens_from_db, lemmas_from_db = get_vocab_in_db()\n",
    "    \n",
    "    if tokens_from_db.shape == (0,0):\n",
    "        dict_list_tokens_to_insert = []\n",
    "        for index, token in zip(text_tokens_series.index,text_tokens_series.values):\n",
    "            dict_list_tokens_to_insert.append({\"_key\":f'token{index}',\n",
    "                                               'token':token})\n",
    "        db.collection('tokens').import_bulk(dict_list_tokens_to_insert)\n",
    "\n",
    "        dict_list_lemmas_to_insert = []\n",
    "        for index, lemma in zip(text_lemmas_series.index,text_lemmas_series.values):\n",
    "            dict_list_lemmas_to_insert.append({\"_key\":f'lemma{index}',\n",
    "                                               'lemma':lemma})\n",
    "        db.collection('lemmas').import_bulk(dict_list_lemmas_to_insert)\n",
    "\n",
    "    else :\n",
    "        new_vocab_tokens = keep_only_new_vocab(text_tokens_series,tokens_from_db)\n",
    "        new_vocab_lemmas = keep_only_new_vocab(text_lemmas_series,lemmas_from_db)\n",
    "\n",
    "        new_vocab_tokens.index = new_vocab_tokens.index + tokens_from_db.shape[0] + 1\n",
    "        new_vocab_lemmas.index = new_vocab_lemmas.index + lemmas_from_db.shape[0] + 1\n",
    "\n",
    "        dict_list_tokens_to_insert = []\n",
    "        for index, token in zip(new_vocab_tokens.index,new_vocab_tokens.values):\n",
    "            dict_list_tokens_to_insert.append({\"_key\":f'token{index}',\n",
    "                                               'token':token})\n",
    "        db.collection('tokens').import_bulk(dict_list_tokens_to_insert)\n",
    "\n",
    "        dict_list_lemmas_to_insert = []\n",
    "        for index, lemma in zip(new_vocab_lemmas.index,new_vocab_lemmas.values):\n",
    "            dict_list_lemmas_to_insert.append({\"_key\":f'lemma{index}',\n",
    "                                               'lemma':lemma})\n",
    "        db.collection('lemmas').import_bulk(dict_list_lemmas_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c0d78688-1a0f-4426-97eb-335eac3d9908",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_vocab_to_db(processed_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d329c5a5-53a2-4ec6-8677-310480a98a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_vocab_to_db(processed_docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d7af4c-274a-4919-a6f0-e75cf0cfd109",
   "metadata": {},
   "source": [
    "### Relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df0bcc3-81ca-4f01-831c-5cb82d7fdfa3",
   "metadata": {},
   "source": [
    "Les 3 types de relations dans le graphe, ce qu'elles connectent et ce qu'elles contiennent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71851b-a31c-4225-9668-76d7ac2e711a",
   "metadata": {},
   "source": [
    "phrases aux docs :\n",
    "- faire un call sur les phrases de la db \n",
    "- clé from = la clé de chaque phrase \n",
    "- clé to = la première partie de la clé "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4cb76a79-fe26-400f-9305-6048d0bdcd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_keys = pd.Series(list(db.aql.execute('''\n",
    "                        FOR doc in sentences\n",
    "                        return doc._key\n",
    "                        ''')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cca4018b-bf4d-410e-b676-364d9883dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_number_for_sentences = sentences_keys.str.extract('(\\d+)')[0]\n",
    "sentences_number = sentences_keys.str.extract('\\D+\\d+\\D+(\\d+)')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "802c89c6-0961-4bfa-b0e1-595b07c4795b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       doc0sent0\n",
       "1       doc0sent1\n",
       "2      doc0sent11\n",
       "3      doc0sent12\n",
       "4      doc0sent13\n",
       "          ...    \n",
       "531    doc1sent95\n",
       "532    doc1sent96\n",
       "533    doc1sent97\n",
       "534    doc1sent98\n",
       "535    doc1sent99\n",
       "Length: 536, dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eb3654-3ab1-42c0-8aab-098ea84b82f3",
   "metadata": {},
   "source": [
    "is_from :\n",
    "- les phrases aux documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d0a652de-f45f-4161-874d-266cd216b383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': False,\n",
       " 'created': 536,\n",
       " 'errors': 0,\n",
       " 'empty': 0,\n",
       " 'updated': 0,\n",
       " 'ignored': 0,\n",
       " 'details': []}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_is_from_sent_doc_to_insert = []\n",
    "for sentence_key, doc_number, sentence_number in zip(sentences_keys.values,doc_number_for_sentences,sentences_number.values):\n",
    "    dict_is_from_sent_doc_to_insert.append({'_from':sentence_key,\n",
    "                                            '_to':f'doc{doc_number}',\n",
    "                                            'sentence_number': sentence_number})\n",
    "db.collection('is_from').import_bulk(dict_is_from_sent_doc_to_insert,\n",
    "                                     from_prefix='sentences/',\n",
    "                                     to_prefix='docs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e931051c-e39f-4a20-9b2e-672b9152dd57",
   "metadata": {},
   "source": [
    "is_from :\n",
    "- les tokens aux phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72783cc-9582-40cb-b095-f33d754898e5",
   "metadata": {},
   "source": [
    "Extraction d'une forme tabulaire de la structure syntagmatique.\n",
    "  - Le processus est itéré sur l'ensemble des phrases du document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "89cb8bf6-5187-455a-bd20-cb39ca0d0410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dependancy_df(processed_text,doc_number):\n",
    "    token_text, token_dep, token_head_text, token_head_pos, sentence_number = [], [], [], [],[]\n",
    "\n",
    "    for count, sentence in enumerate(processed_text.sents):\n",
    "        for token in sentence:\n",
    "            if not token.is_punct and not token.is_stop and not token.is_space and not token.is_digit:\n",
    "                token_text.append(token.text)\n",
    "                token_dep.append(token.dep_), \n",
    "                token_head_text.append(token.head.text), \n",
    "                token_head_pos.append( token.head.pos_)\n",
    "                sentence_number.append(f'doc{doc_number}sent{count}')\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame({'token':token_text,\n",
    "                       'dep':token_dep,\n",
    "                       'head_text':token_head_text,\n",
    "                       'head_pos':token_head_pos,\n",
    "                       'sentence_number':sentence_number})   \n",
    "    df = df[df['head_pos']!=\"SPACE\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d0b610dd-85a7-4c19-a3bf-59006d2358f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependancy_df = create_dependancy_df(processed_docs[0],0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1bed463d-028a-49e7-9c9f-c5e1e54c13f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>dep</th>\n",
       "      <th>head_text</th>\n",
       "      <th>head_pos</th>\n",
       "      <th>sentence_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faire</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>vais</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inutilement</td>\n",
       "      <td>advmod</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>durer</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suspense</td>\n",
       "      <td>obj</td>\n",
       "      <td>durer</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>réponse</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>oui</td>\n",
       "      <td>ADV</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>évoluer</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>peuvent</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>cesse</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>jamais</td>\n",
       "      <td>advmod</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>étonner</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>intéresse</td>\n",
       "      <td>conj</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           token     dep head_text head_pos sentence_number\n",
       "0          faire   xcomp      vais     VERB       doc0sent0\n",
       "1    inutilement  advmod     faire     VERB       doc0sent0\n",
       "2          durer   xcomp     faire     VERB       doc0sent0\n",
       "3       suspense     obj     durer     VERB       doc0sent0\n",
       "4        réponse   nsubj       oui      ADV       doc0sent0\n",
       "..           ...     ...       ...      ...             ...\n",
       "106      évoluer   xcomp   peuvent     VERB      doc0sent15\n",
       "107        cesse    ROOT     cesse     VERB      doc0sent16\n",
       "108       jamais  advmod     cesse     VERB      doc0sent16\n",
       "109      étonner   xcomp     cesse     VERB      doc0sent16\n",
       "110    intéresse    conj     cesse     VERB      doc0sent16\n",
       "\n",
       "[111 rows x 5 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependancy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "26d92d50-945c-4e5f-806e-d55f4ffbd604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faire</td>\n",
       "      <td>token0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inutilement</td>\n",
       "      <td>token1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>durer</td>\n",
       "      <td>token2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suspense</td>\n",
       "      <td>token3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>réponse</td>\n",
       "      <td>token4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2855</th>\n",
       "      <td>florence</td>\n",
       "      <td>token2856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>lefresne</td>\n",
       "      <td>token2857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2857</th>\n",
       "      <td>vecteurs</td>\n",
       "      <td>token2858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2858</th>\n",
       "      <td>structurelle</td>\n",
       "      <td>token2859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2859</th>\n",
       "      <td>juillet</td>\n",
       "      <td>token2860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2860 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word        key\n",
       "0            faire     token0\n",
       "1      inutilement     token1\n",
       "2            durer     token2\n",
       "3         suspense     token3\n",
       "4          réponse     token4\n",
       "...            ...        ...\n",
       "2855      florence  token2856\n",
       "2856      lefresne  token2857\n",
       "2857      vecteurs  token2858\n",
       "2858  structurelle  token2859\n",
       "2859       juillet  token2860\n",
       "\n",
       "[2860 rows x 2 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_table_tokens_in_db = get_vocab_in_db()[0]\n",
    "vocab_table_tokens_in_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c83df8ef-0131-4bc5-94de-7825c377102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_from_sentence_table = dependancy_df.merge(vocab_table_tokens_in_db, \n",
    "                                                        left_on='token', \n",
    "                                                        right_on='word')\\\n",
    "                                                        .rename(columns={'key':'token_key'})\\\n",
    "                                                        .drop('word',axis=1)\n",
    "\n",
    "dependancy_table_for_insert = token_from_sentence_table.merge(vocab_table_tokens_in_db,\n",
    "                                                        left_on='head_text',\n",
    "                                                        right_on='word')\\\n",
    "                                                        .rename(columns={'key':'head_text_key'})\\\n",
    "                                                        .drop('word',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "07871e6e-0267-4aef-a0ba-75bda5ee4caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>dep</th>\n",
       "      <th>head_text</th>\n",
       "      <th>head_pos</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>token_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faire</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>vais</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "      <td>token0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faire</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent9</td>\n",
       "      <td>token0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inutilement</td>\n",
       "      <td>advmod</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "      <td>token1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>durer</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "      <td>token2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suspense</td>\n",
       "      <td>obj</td>\n",
       "      <td>durer</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "      <td>token3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>évoluer</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>peuvent</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent15</td>\n",
       "      <td>token87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>cesse</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "      <td>token88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>jamais</td>\n",
       "      <td>advmod</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "      <td>token89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>étonner</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "      <td>token90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>intéresse</td>\n",
       "      <td>conj</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "      <td>token91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           token     dep head_text head_pos sentence_number token_key\n",
       "0          faire   xcomp      vais     VERB       doc0sent0    token0\n",
       "1          faire    ROOT     faire     VERB       doc0sent9    token0\n",
       "2    inutilement  advmod     faire     VERB       doc0sent0    token1\n",
       "3          durer   xcomp     faire     VERB       doc0sent0    token2\n",
       "4       suspense     obj     durer     VERB       doc0sent0    token3\n",
       "..           ...     ...       ...      ...             ...       ...\n",
       "106      évoluer   xcomp   peuvent     VERB      doc0sent15   token87\n",
       "107        cesse    ROOT     cesse     VERB      doc0sent16   token88\n",
       "108       jamais  advmod     cesse     VERB      doc0sent16   token89\n",
       "109      étonner   xcomp     cesse     VERB      doc0sent16   token90\n",
       "110    intéresse    conj     cesse     VERB      doc0sent16   token91\n",
       "\n",
       "[111 rows x 6 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_from_sentence_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "59b5c3dd-f88e-4ab2-9957-4f2fff059af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': False,\n",
       " 'created': 111,\n",
       " 'errors': 0,\n",
       " 'empty': 0,\n",
       " 'updated': 0,\n",
       " 'ignored': 0,\n",
       " 'details': []}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_is_from_sent_token_to_insert = []\n",
    "for token_key,sentence_number in zip(token_from_sentence_table['token_key'],token_from_sentence_table['sentence_number'] ):\n",
    "    dict_is_from_sent_token_to_insert.append({'_from':token_key,\n",
    "                                            '_to':sentence_number})\n",
    "db.collection('is_from').import_bulk(dict_is_from_sent_token_to_insert,\n",
    "                                     from_prefix='tokens/',\n",
    "                                     to_prefix='sentences/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "27b2bee8-bf66-435c-b836-88ae5f3fc81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       doc0sent0\n",
       "1       doc0sent9\n",
       "2       doc0sent0\n",
       "3       doc0sent0\n",
       "4       doc0sent0\n",
       "          ...    \n",
       "106    doc0sent15\n",
       "107    doc0sent16\n",
       "108    doc0sent16\n",
       "109    doc0sent16\n",
       "110    doc0sent16\n",
       "Name: sentence_number, Length: 111, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_from_sentence_table['sentence_number']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17299a75-99a9-4815-85a3-c371481f08fc",
   "metadata": {},
   "source": [
    "syntagmatic_link :\n",
    "- les tokens aux tokens \n",
    "    - le type de relation grammaticale\n",
    "    - l'ID de la phrase contenant cette relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f7c3175b-9cc1-49d0-b5df-8669b2ddc897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>dep</th>\n",
       "      <th>head_text</th>\n",
       "      <th>head_pos</th>\n",
       "      <th>sentence_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faire</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>vais</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inutilement</td>\n",
       "      <td>advmod</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>durer</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suspense</td>\n",
       "      <td>obj</td>\n",
       "      <td>durer</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>réponse</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>oui</td>\n",
       "      <td>ADV</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>évoluer</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>peuvent</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>cesse</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>jamais</td>\n",
       "      <td>advmod</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>étonner</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>intéresse</td>\n",
       "      <td>conj</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           token     dep head_text head_pos sentence_number\n",
       "0          faire   xcomp      vais     VERB       doc0sent0\n",
       "1    inutilement  advmod     faire     VERB       doc0sent0\n",
       "2          durer   xcomp     faire     VERB       doc0sent0\n",
       "3       suspense     obj     durer     VERB       doc0sent0\n",
       "4        réponse   nsubj       oui      ADV       doc0sent0\n",
       "..           ...     ...       ...      ...             ...\n",
       "106      évoluer   xcomp   peuvent     VERB      doc0sent15\n",
       "107        cesse    ROOT     cesse     VERB      doc0sent16\n",
       "108       jamais  advmod     cesse     VERB      doc0sent16\n",
       "109      étonner   xcomp     cesse     VERB      doc0sent16\n",
       "110    intéresse    conj     cesse     VERB      doc0sent16\n",
       "\n",
       "[111 rows x 5 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependancy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b1222-3b8b-417c-b2d2-fe3821c30601",
   "metadata": {},
   "source": [
    "Merge des tokens avec les id correspondants dans la db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "14bc159a-02d4-430d-b4b9-bcc7865282d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>dep</th>\n",
       "      <th>head_text</th>\n",
       "      <th>head_pos</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>token_key</th>\n",
       "      <th>head_text_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faire</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent9</td>\n",
       "      <td>token0</td>\n",
       "      <td>token0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inutilement</td>\n",
       "      <td>advmod</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "      <td>token1</td>\n",
       "      <td>token0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>durer</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "      <td>token2</td>\n",
       "      <td>token0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>peut-être</td>\n",
       "      <td>advmod</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent9</td>\n",
       "      <td>token38</td>\n",
       "      <td>token0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pied</td>\n",
       "      <td>obj</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent9</td>\n",
       "      <td>token39</td>\n",
       "      <td>token0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>aborder</td>\n",
       "      <td>acl</td>\n",
       "      <td>manière</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>doc0sent15</td>\n",
       "      <td>token85</td>\n",
       "      <td>token84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>cesse</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "      <td>token88</td>\n",
       "      <td>token88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>jamais</td>\n",
       "      <td>advmod</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "      <td>token89</td>\n",
       "      <td>token88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>étonner</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "      <td>token90</td>\n",
       "      <td>token88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>intéresse</td>\n",
       "      <td>conj</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent16</td>\n",
       "      <td>token91</td>\n",
       "      <td>token88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          token     dep head_text head_pos sentence_number token_key  \\\n",
       "0         faire    ROOT     faire     VERB       doc0sent9    token0   \n",
       "1   inutilement  advmod     faire     VERB       doc0sent0    token1   \n",
       "2         durer   xcomp     faire     VERB       doc0sent0    token2   \n",
       "3     peut-être  advmod     faire     VERB       doc0sent9   token38   \n",
       "4          pied     obj     faire     VERB       doc0sent9   token39   \n",
       "..          ...     ...       ...      ...             ...       ...   \n",
       "77      aborder     acl   manière     NOUN      doc0sent15   token85   \n",
       "78        cesse    ROOT     cesse     VERB      doc0sent16   token88   \n",
       "79       jamais  advmod     cesse     VERB      doc0sent16   token89   \n",
       "80      étonner   xcomp     cesse     VERB      doc0sent16   token90   \n",
       "81    intéresse    conj     cesse     VERB      doc0sent16   token91   \n",
       "\n",
       "   head_text_key  \n",
       "0         token0  \n",
       "1         token0  \n",
       "2         token0  \n",
       "3         token0  \n",
       "4         token0  \n",
       "..           ...  \n",
       "77       token84  \n",
       "78       token88  \n",
       "79       token88  \n",
       "80       token88  \n",
       "81       token88  \n",
       "\n",
       "[82 rows x 7 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependancy_table_for_insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cd450742-8fb0-4608-9f00-51d12f615349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': False,\n",
       " 'created': 82,\n",
       " 'errors': 0,\n",
       " 'empty': 0,\n",
       " 'updated': 0,\n",
       " 'ignored': 0,\n",
       " 'details': []}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_syntagmatic_link_to_insert = []\n",
    "for head_text_key, token_key, dep_relation, head_pos_tag, sentence_number in zip(dependancy_table_for_insert['head_text_key'],\n",
    "                                                                                 dependancy_table_for_insert['token_key'],\n",
    "                                                                                 dependancy_table_for_insert['dep'],\n",
    "                                                                                 dependancy_table_for_insert['head_pos'],\n",
    "                                                                                 dependancy_table_for_insert['sentence_number']):\n",
    "    dict_syntagmatic_link_to_insert.append({'_from':head_text_key,\n",
    "                                            '_to':token_key,\n",
    "                                            'dep_relation':dep_relation,\n",
    "                                            'head_pos_tag':head_pos_tag,\n",
    "                                            'from_sentence_number':sentence_number})\n",
    "db.collection('syntagmatic_link').import_bulk(dict_syntagmatic_link_to_insert,\n",
    "                                             from_prefix='tokens/',\n",
    "                                             to_prefix='tokens/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93900680-0645-474c-9a78-13fc34cf7166",
   "metadata": {},
   "source": [
    "contracts_to :\n",
    "- Les tokens aux lemmes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57635bce-3f06-43ad-99bc-b32ddaed1c52",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8533de3-3f27-489e-af73-43e3593715f8",
   "metadata": {},
   "source": [
    "# TODO :\n",
    "- Insérer une logique de check database / non répétition des insertions lors de l'insertion de nouvelles phrases\n",
    "- Assigner dans is_from entre les mots et les phrases la clé de la phrase (doc0sent20) -check\n",
    "- connecter les lemmes avec les tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbff409-4143-4ddf-9c63-4ae200feee0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
