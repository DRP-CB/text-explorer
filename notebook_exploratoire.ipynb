{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc6c7873-5841-4927-ad07-c4d9c9643b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import pyArango\n",
    "import os\n",
    "from os import path\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "from arango import ArangoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102ea8a7-7052-4641-af64-9fea04316dff",
   "metadata": {},
   "source": [
    "Initialisation de la connection avec la base de données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7232caf-3eff-4b8b-bd0e-56ba3bc44a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the client for ArangoDB.\n",
    "client = ArangoClient(hosts=\"http://localhost:8529\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cb3f359-7e8e-49bf-87a4-9fd3b76a11ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_db = client.db('_system', username='root',password='root')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987f104-3e32-4c55-9e8c-6ea4f499644e",
   "metadata": {},
   "source": [
    "si besoin de repartir à zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c6106a-66bb-47dd-a771-5881e9e203c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_db.delete_database('text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5bb57b-0e9e-411b-8cc1-502108436582",
   "metadata": {},
   "source": [
    "Check si la base de données existe déjà et création si besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a20a9e06-ed07-4f5b-8b47-00d6041d6040",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not sys_db.has_database('text'):\n",
    "    \n",
    "    sys_db.create_database('text')\n",
    "    db = client.db(\"text\", username=\"root\", password=\"root\")\n",
    "    graph = db.create_graph('text_explorer')\n",
    "    # création des collections\n",
    "    docs = graph.create_vertex_collection('docs')\n",
    "    sentences = graph.create_vertex_collection('sentences')\n",
    "    tokens = graph.create_vertex_collection('tokens')\n",
    "    lemmas = graph.create_vertex_collection('lemmas')\n",
    "    # création des arrêtes\n",
    "    is_from = graph.create_edge_definition(\n",
    "        edge_collection='is_from',\n",
    "        from_vertex_collections=['sentences'],\n",
    "        to_vertex_collections=['docs']\n",
    "    )\n",
    "    contracts_to = graph.create_edge_definition(\n",
    "        edge_collection='contracts_to',\n",
    "        from_vertex_collections=['tokens'],\n",
    "        to_vertex_collections=['lemmas']\n",
    "    )\n",
    "    syntagmatic_link = graph.create_edge_definition(\n",
    "        edge_collection='syntagmatic_link',\n",
    "        from_vertex_collections=['tokens'],\n",
    "        to_vertex_collections=['tokens']\n",
    "    )\n",
    "else:\n",
    "    db = client.db(\"text\", username=\"root\", password=\"root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57df3b3-4e03-4e6b-9347-a372fd5f0638",
   "metadata": {},
   "source": [
    "L'objectif maintenant est de remplir ces champs avec les données extraites depuis le texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e35a3-c60c-44eb-ab33-aef3f046f55b",
   "metadata": {},
   "source": [
    "Récupération des documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551cdde1-7d69-4e98-9c99-193778339df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(path):\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        return(f.read().replace('\\n',' '))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e859a1b1-d3d1-4064-b33e-30bf440d5125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/paul/projects/text explorer'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path = os.getcwd()\n",
    "dir_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d540b4d-726e-4254-bf5a-719c699d4251",
   "metadata": {},
   "source": [
    "Liste des fichiers présents dans le dossier choisi pour l'analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "405e5599-89c9-414b-8e90-8aa71d951494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/paul/projects/text_for_app/jean_blog.txt',\n",
       " '/home/paul/projects/text_for_app/emploi étudiant et inégalités sociales.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('/home/paul/projects/text_for_app/*.{}'.format('txt'))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6ef6cb5-0757-4017-a3b8-d14be62a678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_from_path(path):\n",
    "    return os.path.normpath(path).split(os.sep)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "490f3921-3654-4653-a4c4-3b59bee566a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>doc_name</th>\n",
       "      <th>doc_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/paul/projects/text_for_app/jean_blog.txt</td>\n",
       "      <td>jean_blog.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/paul/projects/text_for_app/emploi étudia...</td>\n",
       "      <td>emploi étudiant et inégalités sociales.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath  \\\n",
       "0     /home/paul/projects/text_for_app/jean_blog.txt   \n",
       "1  /home/paul/projects/text_for_app/emploi étudia...   \n",
       "\n",
       "                                     doc_name  doc_number  \n",
       "0                               jean_blog.txt           0  \n",
       "1  emploi étudiant et inégalités sociales.txt           1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = pd.DataFrame({'filepath':files,\n",
    "                          'doc_name':[get_filename_from_path(filepath) for filepath in files],\n",
    "                          'doc_number':list(range(0,len(files)))})\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11549c59-b792-4492-b26b-be12ac62163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_docs(documents_found):\n",
    "    in_db_doclist = pd.DataFrame(list(db.aql.execute('''FOR doc in docs RETURN doc''')))\n",
    "    if in_db_doclist.shape == (0,0):\n",
    "        pass\n",
    "    else :\n",
    "        documents_found = documents_found[~documents_found['doc_name'].isin(in_db_doclist['doc_name'])]\n",
    "        last_doc_number = in_db_doclist['doc_number'].max()\n",
    "        documents_found['doc_number'] = documents_found['doc_number'] + last_doc_number + 1\n",
    "    \n",
    "    dict_list_documents_to_insert = []\n",
    "    for doc_name, number, path  in zip(documents_found['doc_name'], documents_found['doc_number'], documents_found['filepath']):\n",
    "        dict_list_documents_to_insert.append({'_key':f'doc{number}',\n",
    "                                                'doc_name':doc_name,\n",
    "                                                'doc_path':path,\n",
    "                                                'doc_number':number,\n",
    "                                                'processed':'False'})\n",
    "    db.collection('docs').import_bulk(dict_list_documents_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72c053c1-169a-443b-a1c3-1d55fdf44a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_docs(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b5745-ffc1-475f-a3f7-b8bf2e4efffe",
   "metadata": {},
   "source": [
    "# Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "445a2ee3-c54b-4013-9c62-47687b08ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dfbdeb-8e90-4c45-bf06-3855041d95eb",
   "metadata": {},
   "source": [
    "Traitement des fichiers par le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4464a67-236a-497a-ae38-c562907e975d",
   "metadata": {},
   "source": [
    "## se renseigner sur comment faire du traitement en batch\n",
    "    - mesurer l'empreinte mémoire de l'opération et ajuster combien de documents en même temps peuvent être traités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4986fa9-3661-4abb-bfe8-9559a3e21789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/paul/projects/text_for_app/jean_blog.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/paul/projects/text_for_app/emploi étudia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  number\n",
       "0     /home/paul/projects/text_for_app/jean_blog.txt       0\n",
       "1  /home/paul/projects/text_for_app/emploi étudia...       1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_to_process_df = pd.DataFrame(\n",
    "                        list(\n",
    "                            db.aql.execute('''\n",
    "                                            FOR doc in docs\n",
    "                                            FILTER doc.processed == 'False'\n",
    "                                            RETURN {path :doc.doc_path, number : doc.doc_number}\n",
    "                                            ''')\n",
    "                            )\n",
    "                        )\n",
    "texts_to_process_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026546dd-8c66-4427-a9ac-c4f5acfcaaec",
   "metadata": {},
   "source": [
    "Voir l'empreinte mémoire de l'opération. Découper l'output des textes à traiter pour ne pas tout fournir direction à l'opération ci dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff4777cf-abc3-492d-b1f5-87ee8d14862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = list(nlp.pipe([get_text(path) for path in texts_to_process_df['path']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee3a1af2-f9ac-404b-b860-3d8f34cc9f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_sentences(processed_doc, doc_number):\n",
    "    dict_sentences_to_insert = []\n",
    "    for sentence_number, sentence in enumerate(processed_doc.sents):\n",
    "        if sentence.text != ' ':\n",
    "            dict_sentences_to_insert.append({'_key':f'doc{doc_number}sent{sentence_number}',\n",
    "                                             'content':sentence.text})\n",
    "        else:\n",
    "            pass\n",
    "    db.collection('sentences').import_bulk(dict_sentences_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f62babab-fea7-4be7-ab70-9121ec1535c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for processed_text, doc_number in zip(processed_docs,texts_to_process_df['number']):\n",
    "    insert_sentences(processed_text,doc_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1dbfb4-6e20-4e19-b6ab-42daca6ef69b",
   "metadata": {},
   "source": [
    "Insertion des tokens et lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bca780-d91b-48bf-b25d-63d08569bc2f",
   "metadata": {},
   "source": [
    "Récupérer le vocabulaire token et lemma de chaque doc et les insérer dans la db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a93447-8583-44dd-aa24-be643f1dc85a",
   "metadata": {},
   "source": [
    "- clé = nombre arbitraire d'insertion\n",
    "- valeur = le token ou lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cc2826-6fc9-4bc4-b35d-bb2a78e05eda",
   "metadata": {},
   "source": [
    "Pour insérer de nouveaux mots par la suite faire une requette sur la table des tokens / lemma et insérer ceux qui ne sont pas déjà présents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e83a71-7889-4bee-a35c-8d170d94c8a0",
   "metadata": {},
   "source": [
    "pour les relations faire une requete par phrase où l'on récupère chaque mot et sa clé associée\n",
    "- relier ensuite les mots entre eux en utilisant les clés et la modalité de cette relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7258a4-a7b8-424b-bc1c-1caed15d051e",
   "metadata": {},
   "source": [
    "Comment relier les phrases aux documents et les mots aux phrases ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6d4cd-cceb-4dc3-8ccf-032e66685e96",
   "metadata": {},
   "source": [
    "### Extraction des correspondances lemmes / tokens depuis le texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d797687f-5d8b-4104-9809-e09303ba3603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_table_from_text(processed_text):\n",
    "    tokens, lemmas = [], []\n",
    "    for token in processed_text:\n",
    "        if not token.is_punct and not token.is_stop and not token.is_space and not token.is_digit:\n",
    "            tokens.append(token.text.lower())\n",
    "            lemmas.append(token.lemma_.lower())\n",
    "    vocab_table = pd.DataFrame({'token':tokens,\n",
    "                                'lemma':lemmas})\n",
    "    return vocab_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a14c80-f90a-48f5-acfe-917848501460",
   "metadata": {},
   "source": [
    "### Construction de deux tables contenant les occurences uniques des lemmes et tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc915556-a603-44c0-a23f-492dd8e59ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_vocabulary(token_lemma_table,word_type):\n",
    "    return(token_lemma_table[word_type].drop_duplicates().reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff11335-c85b-4560-b893-014494f2ac57",
   "metadata": {},
   "source": [
    "### Extraction du vocabulaire existant depuis la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9b780fb-9719-456f-8f9c-b342d80bdeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_in_db():\n",
    "    vocab_tokens_from_db = pd.DataFrame(\n",
    "        list(db.aql.execute(\"\"\"\n",
    "            FOR doc in tokens\n",
    "            RETURN {word :doc.token, key : doc._key}\n",
    "            \"\"\"))\n",
    "        )\n",
    "        \n",
    "    vocab_lemmas_from_db = pd.DataFrame(\n",
    "        list(db.aql.execute(\"\"\"\n",
    "            FOR doc in lemmas\n",
    "            RETURN {word :doc.lemma, key : doc._key}\n",
    "            \"\"\"))\n",
    "        )\n",
    "    return vocab_tokens_from_db, vocab_lemmas_from_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d72934-838b-40a3-a63a-e78fad879ef3",
   "metadata": {},
   "source": [
    "### Comparaison du vocabulaire extrait du texte avec celui déjà contenu dans la db\n",
    "- On ne rajoute que les instances jamais vues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "930abf54-e511-471a-818a-d7264db99b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_only_new_vocab(vocab_from_text,vocab_from_db):\n",
    "    new_vocab = vocab_from_text[~vocab_from_text.isin(vocab_from_db['word'])].reset_index(drop=True)\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c910ef-1fb6-452c-ad1a-7c577befb09b",
   "metadata": {},
   "source": [
    "A refactoriser :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1b90650-55f2-4215-86bb-04abb602f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vocab_to_db(processed_text):\n",
    "    token_to_lemma_table = get_vocab_table_from_text(processed_text)\n",
    "    \n",
    "    text_tokens_series = unique_vocabulary(token_to_lemma_table,'token')\n",
    "    text_lemmas_series = unique_vocabulary(token_to_lemma_table,'lemma')\n",
    "    \n",
    "    tokens_from_db, lemmas_from_db = get_vocab_in_db()\n",
    "    \n",
    "    if tokens_from_db.shape == (0,0):\n",
    "        dict_list_tokens_to_insert = []\n",
    "        for index, token in zip(text_tokens_series.index,text_tokens_series.values):\n",
    "            dict_list_tokens_to_insert.append({\"_key\":f'token{index}',\n",
    "                                               'token':token})\n",
    "        db.collection('tokens').import_bulk(dict_list_tokens_to_insert)\n",
    "\n",
    "        dict_list_lemmas_to_insert = []\n",
    "        for index, lemma in zip(text_lemmas_series.index,text_lemmas_series.values):\n",
    "            dict_list_lemmas_to_insert.append({\"_key\":f'lemma{index}',\n",
    "                                               'lemma':lemma})\n",
    "        db.collection('lemmas').import_bulk(dict_list_lemmas_to_insert)\n",
    "\n",
    "    else :\n",
    "        new_vocab_tokens = keep_only_new_vocab(text_tokens_series,tokens_from_db)\n",
    "        new_vocab_lemmas = keep_only_new_vocab(text_lemmas_series,lemmas_from_db)\n",
    "\n",
    "        new_vocab_tokens.index = new_vocab_tokens.index + tokens_from_db.shape[0] + 1\n",
    "        new_vocab_lemmas.index = new_vocab_lemmas.index + lemmas_from_db.shape[0] + 1\n",
    "\n",
    "        dict_list_tokens_to_insert = []\n",
    "        for index, token in zip(new_vocab_tokens.index,new_vocab_tokens.values):\n",
    "            dict_list_tokens_to_insert.append({\"_key\":f'token{index}',\n",
    "                                               'token':token})\n",
    "        db.collection('tokens').import_bulk(dict_list_tokens_to_insert)\n",
    "\n",
    "        dict_list_lemmas_to_insert = []\n",
    "        for index, lemma in zip(new_vocab_lemmas.index,new_vocab_lemmas.values):\n",
    "            dict_list_lemmas_to_insert.append({\"_key\":f'lemma{index}',\n",
    "                                               'lemma':lemma})\n",
    "        db.collection('lemmas').import_bulk(dict_list_lemmas_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0d78688-1a0f-4426-97eb-335eac3d9908",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_vocab_to_db(processed_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d329c5a5-53a2-4ec6-8677-310480a98a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_vocab_to_db(processed_docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f44509-e423-405c-a25d-edb818c9fd7e",
   "metadata": {},
   "source": [
    "# todo :\n",
    "- insérer les relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "994daafe-1a48-4664-8ba3-f087a5f55c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "9576508d-7c71-4c03-89ed-44da60600268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57635bce-3f06-43ad-99bc-b32ddaed1c52",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ce844-4cad-49b8-86c0-835894eeee0e",
   "metadata": {},
   "source": [
    "Extraction d'une forme tabulaire de la structure syntagmatique.\n",
    "  > Le processus est itéré sur l'ensemble des phrases du document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48820f5e-2670-4abc-869c-acddbac58d55",
   "metadata": {},
   "source": [
    "on obtient une liste de dataframes contenant la structure syntagmatique pour chaque phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bebd5068-1e89-4cd1-887d-c4b6ba162a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dependancy_df_list(processed_text):\n",
    "    df_list = []\n",
    "    for sentence in processed_text.sents:\n",
    "        token_text, token_dep, token_head_text, token_head_pos = [], [], [], []\n",
    "        for token in sentence:\n",
    "            if not token.is_punct and not token.is_stop and not token.is_space:\n",
    "                token_text.append(token.text)\n",
    "                token_dep.append(token.dep_), \n",
    "                token_head_text.append(token.head.text), \n",
    "                token_head_pos.append( token.head.pos_)\n",
    "        df = pd.DataFrame({'token':token_text,\n",
    "                           'dep':token_dep,\n",
    "                           'head_text':token_head_text,\n",
    "                           'head_pos':token_head_pos})    \n",
    "        if not df.empty:\n",
    "            df_list.append(df)\n",
    "        else:\n",
    "            pass\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd14dcb0-c6fd-46bd-b8ad-7ef0efdec824",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1_processed = nlp(get_text(documents['filepath'][0]))\n",
    "file_2_processed = nlp(get_text(documents['filepath'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71c5fc2f-1b44-4cc5-941c-c8529675dc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>dep</th>\n",
       "      <th>head_text</th>\n",
       "      <th>head_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faire</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>vais</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inutilement</td>\n",
       "      <td>advmod</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>durer</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suspense</td>\n",
       "      <td>obj</td>\n",
       "      <td>durer</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>réponse</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>oui</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>oui</td>\n",
       "      <td>parataxis</td>\n",
       "      <td>vais</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         token        dep head_text head_pos\n",
       "0        faire      xcomp      vais     VERB\n",
       "1  inutilement     advmod     faire     VERB\n",
       "2        durer      xcomp     faire     VERB\n",
       "3     suspense        obj     durer     VERB\n",
       "4      réponse      nsubj       oui      ADV\n",
       "5          oui  parataxis      vais     VERB"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_dependancy_df_list(file_1_processed)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba3d25-2d6d-4642-b786-ad4c943e014d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
