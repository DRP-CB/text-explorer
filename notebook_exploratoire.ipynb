{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc6c7873-5841-4927-ad07-c4d9c9643b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/projects/text explorer/text_explorer/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/paul/projects/text explorer/text_explorer/lib/python3.8/site-packages/torch/cuda/__init__.py:82: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "import os\n",
    "from os import path\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "from arango import ArangoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102ea8a7-7052-4641-af64-9fea04316dff",
   "metadata": {},
   "source": [
    "Initialisation de la connection avec la base de données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7232caf-3eff-4b8b-bd0e-56ba3bc44a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the client for ArangoDB.\n",
    "client = ArangoClient(hosts=\"http://localhost:8529\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b11e2bc-0128-4456-80ea-a083babc53ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_db = client.db('_system', username='root',password='root')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987f104-3e32-4c55-9e8c-6ea4f499644e",
   "metadata": {},
   "source": [
    "si besoin de repartir à zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c6106a-66bb-47dd-a771-5881e9e203c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_db.delete_database('text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5bb57b-0e9e-411b-8cc1-502108436582",
   "metadata": {},
   "source": [
    "Check si la base de données existe déjà et création si besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a20a9e06-ed07-4f5b-8b47-00d6041d6040",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not sys_db.has_database('text'):\n",
    "    \n",
    "    sys_db.create_database('text')\n",
    "    db = client.db(\"text\", username=\"root\", password=\"root\")\n",
    "    graph = db.create_graph('text_explorer')\n",
    "    # création des collections\n",
    "    docs = graph.create_vertex_collection('docs')\n",
    "    sentences = graph.create_vertex_collection('sentences')\n",
    "    tokens = graph.create_vertex_collection('tokens')\n",
    "    lemmas = graph.create_vertex_collection('lemmas')\n",
    "    # création des arrêtes\n",
    "    is_from = graph.create_edge_definition(\n",
    "        edge_collection='is_from',\n",
    "        from_vertex_collections=['sentences','tokens'],\n",
    "        to_vertex_collections=['docs','sentences']\n",
    "    )\n",
    "    contracts_to = graph.create_edge_definition(\n",
    "        edge_collection='contracts_to',\n",
    "        from_vertex_collections=['tokens'],\n",
    "        to_vertex_collections=['lemmas']\n",
    "    )\n",
    "    syntagmatic_link = graph.create_edge_definition(\n",
    "        edge_collection='syntagmatic_link',\n",
    "        from_vertex_collections=['tokens'],\n",
    "        to_vertex_collections=['tokens']\n",
    "    )\n",
    "else:\n",
    "    db = client.db(\"text\", username=\"root\", password=\"root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57df3b3-4e03-4e6b-9347-a372fd5f0638",
   "metadata": {},
   "source": [
    "L'objectif maintenant est de remplir ces champs avec les données extraites depuis le texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e35a3-c60c-44eb-ab33-aef3f046f55b",
   "metadata": {},
   "source": [
    "Récupération des documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551cdde1-7d69-4e98-9c99-193778339df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(path):\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        return(f.read().replace('\\n',' '))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e859a1b1-d3d1-4064-b33e-30bf440d5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir_path = os.getcwd()\n",
    "#dir_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d540b4d-726e-4254-bf5a-719c699d4251",
   "metadata": {},
   "source": [
    "Liste des fichiers présents dans le dossier choisi pour l'analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "405e5599-89c9-414b-8e90-8aa71d951494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/paul/projects/text_for_app/jean_blog.txt',\n",
       " '/home/paul/projects/text_for_app/emploi étudiant et inégalités sociales.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob('/home/paul/projects/text_for_app/*.{}'.format('txt'))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6ef6cb5-0757-4017-a3b8-d14be62a678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_from_path(path):\n",
    "    return os.path.normpath(path).split(os.sep)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "490f3921-3654-4653-a4c4-3b59bee566a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>doc_name</th>\n",
       "      <th>doc_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/paul/projects/text_for_app/jean_blog.txt</td>\n",
       "      <td>jean_blog.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/paul/projects/text_for_app/emploi étudia...</td>\n",
       "      <td>emploi étudiant et inégalités sociales.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath  \\\n",
       "0     /home/paul/projects/text_for_app/jean_blog.txt   \n",
       "1  /home/paul/projects/text_for_app/emploi étudia...   \n",
       "\n",
       "                                     doc_name  doc_number  \n",
       "0                               jean_blog.txt           0  \n",
       "1  emploi étudiant et inégalités sociales.txt           1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = pd.DataFrame({'filepath':files,\n",
    "                          'doc_name':[get_filename_from_path(filepath) for filepath in files],\n",
    "                          'doc_number':list(range(0,len(files)))})\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11549c59-b792-4492-b26b-be12ac62163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_docs(documents_found):\n",
    "    in_db_doclist = pd.DataFrame(list(db.aql.execute('''FOR doc in docs RETURN doc''')))\n",
    "    if in_db_doclist.shape == (0,0):\n",
    "        pass\n",
    "    else :\n",
    "        documents_found = documents_found[~documents_found['doc_name'].isin(in_db_doclist['doc_name'])]\n",
    "        last_doc_number = in_db_doclist['doc_number'].max()\n",
    "        documents_found['doc_number'] = documents_found['doc_number'] + last_doc_number + 1\n",
    "    \n",
    "    dict_list_documents_to_insert = []\n",
    "    for doc_name, number, path  in zip(documents_found['doc_name'], documents_found['doc_number'], documents_found['filepath']):\n",
    "        dict_list_documents_to_insert.append({'_key':f'doc{number}',\n",
    "                                                'doc_name':doc_name,\n",
    "                                                'doc_path':path,\n",
    "                                                'doc_number':number,\n",
    "                                                'processed':'False'})\n",
    "    db.collection('docs').import_bulk(dict_list_documents_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72c053c1-169a-443b-a1c3-1d55fdf44a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_docs(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b5745-ffc1-475f-a3f7-b8bf2e4efffe",
   "metadata": {},
   "source": [
    "# Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "445a2ee3-c54b-4013-9c62-47687b08ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_dep_news_trf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dfbdeb-8e90-4c45-bf06-3855041d95eb",
   "metadata": {},
   "source": [
    "Traitement des fichiers par le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4464a67-236a-497a-ae38-c562907e975d",
   "metadata": {},
   "source": [
    "## se renseigner sur comment faire du traitement en batch\n",
    "    - mesurer l'empreinte mémoire de l'opération et ajuster combien de documents en même temps peuvent être traités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4986fa9-3661-4abb-bfe8-9559a3e21789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/paul/projects/text_for_app/jean_blog.txt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/paul/projects/text_for_app/emploi étudia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  number\n",
       "0     /home/paul/projects/text_for_app/jean_blog.txt       0\n",
       "1  /home/paul/projects/text_for_app/emploi étudia...       1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_to_process_df = pd.DataFrame(\n",
    "                        list(\n",
    "                            db.aql.execute('''\n",
    "                                            FOR doc in docs\n",
    "                                            FILTER doc.processed == 'False'\n",
    "                                            RETURN {path :doc.doc_path, number : doc.doc_number}\n",
    "                                            ''')\n",
    "                            )\n",
    "                        )\n",
    "texts_to_process_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026546dd-8c66-4427-a9ac-c4f5acfcaaec",
   "metadata": {},
   "source": [
    "Voir l'empreinte mémoire de l'opération. Découper l'output des textes à traiter pour ne pas tout fournir directement à l'opération ci dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5fc9bec7-c713-47ce-9539-5b24e87d925e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-48:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m processed_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mget_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtexts_to_process_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mn_process\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/text explorer/text_explorer/lib/python3.8/site-packages/spacy/language.py:1576\u001b[0m, in \u001b[0;36mLanguage.pipe\u001b[0;34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[0m\n\u001b[1;32m   1574\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m pipes:\n\u001b[1;32m   1575\u001b[0m         docs \u001b[38;5;241m=\u001b[39m pipe(docs)\n\u001b[0;32m-> 1576\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[1;32m   1577\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m doc\n",
      "File \u001b[0;32m~/projects/text explorer/text_explorer/lib/python3.8/site-packages/spacy/language.py:1631\u001b[0m, in \u001b[0;36mLanguage._multiprocessing_pipe\u001b[0;34m(self, texts, pipes, n_process, batch_size)\u001b[0m\n\u001b[1;32m   1627\u001b[0m byte_tuples \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m   1628\u001b[0m     recv\u001b[38;5;241m.\u001b[39mrecv() \u001b[38;5;28;01mfor\u001b[39;00m recv \u001b[38;5;129;01min\u001b[39;00m cycle(bytedocs_recv_ch)\n\u001b[1;32m   1629\u001b[0m )\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (_, (byte_doc, byte_context, byte_error)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m   1632\u001b[0m         \u001b[38;5;28mzip\u001b[39m(raw_texts, byte_tuples), \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1633\u001b[0m     ):\n\u001b[1;32m   1634\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m byte_doc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1635\u001b[0m             doc \u001b[38;5;241m=\u001b[39m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(byte_doc)\n",
      "File \u001b[0;32m~/projects/text explorer/text_explorer/lib/python3.8/site-packages/spacy/language.py:1628\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1623\u001b[0m     proc\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;66;03m# Cycle channels not to break the order of docs.\u001b[39;00m\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;66;03m# The received object is a batch of byte-encoded docs, so flatten them with chain.from_iterable.\u001b[39;00m\n\u001b[1;32m   1627\u001b[0m byte_tuples \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m-> 1628\u001b[0m     \u001b[43mrecv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m recv \u001b[38;5;129;01min\u001b[39;00m cycle(bytedocs_recv_ch)\n\u001b[1;32m   1629\u001b[0m )\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1631\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (_, (byte_doc, byte_context, byte_error)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m   1632\u001b[0m         \u001b[38;5;28mzip\u001b[39m(raw_texts, byte_tuples), \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1633\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/multiprocessing/connection.py:250\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 250\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "processed_docs = list(nlp.pipe([get_text(path) for path in texts_to_process_df['path']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "406a4116-c4ba-4330-ad5c-41fa0442d9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0m_AnyContext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mas_tuples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdisable\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_process\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0m_AnyContext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Process texts as a stream, and yield `Doc` objects in order.\n",
       "\n",
       "texts (Iterable[Union[str, Doc]]): A sequence of texts or docs to\n",
       "    process.\n",
       "as_tuples (bool): If set to True, inputs should be a sequence of\n",
       "    (text, context) tuples. Output will then be a sequence of\n",
       "    (doc, context) tuples. Defaults to False.\n",
       "batch_size (Optional[int]): The number of texts to buffer.\n",
       "disable (List[str]): Names of the pipeline components to disable.\n",
       "component_cfg (Dict[str, Dict]): An optional dictionary with extra keyword\n",
       "    arguments for specific components.\n",
       "n_process (int): Number of processors to process texts. If -1, set `multiprocessing.cpu_count()`.\n",
       "YIELDS (Doc): Documents in the order of the original text.\n",
       "\n",
       "DOCS: https://spacy.io/api/language#pipe\n",
       "\u001b[0;31mFile:\u001b[0m      ~/projects/text explorer/text_explorer/lib/python3.8/site-packages/spacy/language.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp.pipe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee3a1af2-f9ac-404b-b860-3d8f34cc9f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_sentences(processed_doc, doc_number):\n",
    "    dict_sentences_to_insert = []\n",
    "    for sentence_number, sentence in enumerate(processed_doc.sents):\n",
    "        if sentence.text != ' ':\n",
    "            dict_sentences_to_insert.append({'_key':f'doc{doc_number}sent{sentence_number}',\n",
    "                                             'content':sentence.text})\n",
    "        else:\n",
    "            pass\n",
    "    db.collection('sentences').import_bulk(dict_sentences_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0a7c42c-c224-49ac-8b25-307526168618",
   "metadata": {},
   "outputs": [],
   "source": [
    "for processed_text, doc_number in zip(processed_docs,texts_to_process_df['number']):\n",
    "    insert_sentences(processed_text,doc_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1dbfb4-6e20-4e19-b6ab-42daca6ef69b",
   "metadata": {},
   "source": [
    "Insertion des tokens et lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bca780-d91b-48bf-b25d-63d08569bc2f",
   "metadata": {},
   "source": [
    "Récupérer le vocabulaire token et lemma de chaque doc et les insérer dans la db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a93447-8583-44dd-aa24-be643f1dc85a",
   "metadata": {},
   "source": [
    "- clé = nombre arbitraire d'insertion\n",
    "- valeur = le token ou lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cc2826-6fc9-4bc4-b35d-bb2a78e05eda",
   "metadata": {},
   "source": [
    "Pour insérer de nouveaux mots par la suite faire une requette sur la table des tokens / lemma et insérer ceux qui ne sont pas déjà présents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e83a71-7889-4bee-a35c-8d170d94c8a0",
   "metadata": {},
   "source": [
    "pour les relations faire une requete par phrase où l'on récupère chaque mot et sa clé associée\n",
    "- relier ensuite les mots entre eux en utilisant les clés et la modalité de cette relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7258a4-a7b8-424b-bc1c-1caed15d051e",
   "metadata": {},
   "source": [
    "Comment relier les phrases aux documents et les mots aux phrases ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6d4cd-cceb-4dc3-8ccf-032e66685e96",
   "metadata": {},
   "source": [
    "### Extraction des correspondances lemmes / tokens depuis le texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c76d472b-aaaa-4087-b1e1-b3a148bbf13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_table_from_text(processed_text):\n",
    "    tokens, lemmas = [], []\n",
    "    for token in processed_text:\n",
    "        if not token.is_punct and not token.is_stop and not token.is_space and not token.is_digit:\n",
    "            tokens.append(token.text.lower())\n",
    "            lemmas.append(token.lemma_.lower())\n",
    "    vocab_table = pd.DataFrame({'token':tokens,\n",
    "                                'lemma':lemmas})\n",
    "    return vocab_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a14c80-f90a-48f5-acfe-917848501460",
   "metadata": {},
   "source": [
    "### Construction de deux tables contenant les occurences uniques des lemmes et tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc915556-a603-44c0-a23f-492dd8e59ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_vocabulary(token_lemma_table,word_type):\n",
    "    return(token_lemma_table[word_type].drop_duplicates().reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff11335-c85b-4560-b893-014494f2ac57",
   "metadata": {},
   "source": [
    "### Extraction du vocabulaire existant depuis la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9b780fb-9719-456f-8f9c-b342d80bdeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_in_db():\n",
    "    vocab_tokens_from_db = pd.DataFrame(\n",
    "        list(db.aql.execute(\"\"\"\n",
    "            FOR doc in tokens\n",
    "            RETURN {word :doc.token, key : doc._key}\n",
    "            \"\"\"))\n",
    "        )\n",
    "        \n",
    "    vocab_lemmas_from_db = pd.DataFrame(\n",
    "        list(db.aql.execute(\"\"\"\n",
    "            FOR doc in lemmas\n",
    "            RETURN {word :doc.lemma, key : doc._key}\n",
    "            \"\"\"))\n",
    "        )\n",
    "    return vocab_tokens_from_db, vocab_lemmas_from_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d72934-838b-40a3-a63a-e78fad879ef3",
   "metadata": {},
   "source": [
    "### Comparaison du vocabulaire extrait du texte avec celui déjà contenu dans la db\n",
    "- On ne rajoute que les instances jamais vues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "930abf54-e511-471a-818a-d7264db99b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_only_new_vocab(vocab_from_text,vocab_from_db):\n",
    "    new_vocab = vocab_from_text[~vocab_from_text.isin(vocab_from_db['word'])].reset_index(drop=True)\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c910ef-1fb6-452c-ad1a-7c577befb09b",
   "metadata": {},
   "source": [
    "A refactoriser :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1b90650-55f2-4215-86bb-04abb602f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vocab_to_db(processed_text):\n",
    "    token_to_lemma_table = get_vocab_table_from_text(processed_text)\n",
    "    \n",
    "    text_tokens_series = unique_vocabulary(token_to_lemma_table,'token')\n",
    "    text_lemmas_series = unique_vocabulary(token_to_lemma_table,'lemma')\n",
    "    \n",
    "    tokens_from_db, lemmas_from_db = get_vocab_in_db()\n",
    "    \n",
    "    if tokens_from_db.shape == (0,0):\n",
    "        dict_list_tokens_to_insert = []\n",
    "        for index, token in zip(text_tokens_series.index,text_tokens_series.values):\n",
    "            dict_list_tokens_to_insert.append({\"_key\":f'token{index}',\n",
    "                                               'token':token})\n",
    "        db.collection('tokens').import_bulk(dict_list_tokens_to_insert)\n",
    "\n",
    "        dict_list_lemmas_to_insert = []\n",
    "        for index, lemma in zip(text_lemmas_series.index,text_lemmas_series.values):\n",
    "            dict_list_lemmas_to_insert.append({\"_key\":f'lemma{index}',\n",
    "                                               'lemma':lemma})\n",
    "        db.collection('lemmas').import_bulk(dict_list_lemmas_to_insert)\n",
    "\n",
    "    else :\n",
    "        new_vocab_tokens = keep_only_new_vocab(text_tokens_series,tokens_from_db)\n",
    "        new_vocab_lemmas = keep_only_new_vocab(text_lemmas_series,lemmas_from_db)\n",
    "\n",
    "        new_vocab_tokens.index = new_vocab_tokens.index + tokens_from_db.shape[0] + 1\n",
    "        new_vocab_lemmas.index = new_vocab_lemmas.index + lemmas_from_db.shape[0] + 1\n",
    "\n",
    "        dict_list_tokens_to_insert = []\n",
    "        for index, token in zip(new_vocab_tokens.index,new_vocab_tokens.values):\n",
    "            dict_list_tokens_to_insert.append({\"_key\":f'token{index}',\n",
    "                                               'token':token})\n",
    "        db.collection('tokens').import_bulk(dict_list_tokens_to_insert)\n",
    "\n",
    "        dict_list_lemmas_to_insert = []\n",
    "        for index, lemma in zip(new_vocab_lemmas.index,new_vocab_lemmas.values):\n",
    "            dict_list_lemmas_to_insert.append({\"_key\":f'lemma{index}',\n",
    "                                               'lemma':lemma})\n",
    "        db.collection('lemmas').import_bulk(dict_list_lemmas_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d276bbe-4209-4eb8-8b77-60feabb1aa1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faire</td>\n",
       "      <td>faire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inutilement</td>\n",
       "      <td>inutilement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>durer</td>\n",
       "      <td>durer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suspense</td>\n",
       "      <td>suspense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>réponse</td>\n",
       "      <td>réponse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>évoluer</td>\n",
       "      <td>évoluer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>cesse</td>\n",
       "      <td>cesse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>jamais</td>\n",
       "      <td>jamais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>étonner</td>\n",
       "      <td>étonner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>intéresse</td>\n",
       "      <td>intéresse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           token        lemma\n",
       "0          faire        faire\n",
       "1    inutilement  inutilement\n",
       "2          durer        durer\n",
       "3       suspense     suspense\n",
       "4        réponse      réponse\n",
       "..           ...          ...\n",
       "106      évoluer      évoluer\n",
       "107        cesse        cesse\n",
       "108       jamais       jamais\n",
       "109      étonner      étonner\n",
       "110    intéresse    intéresse\n",
       "\n",
       "[111 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vocab_table_from_text(processed_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17a5d00-8f56-4402-b3db-50c3e08f1665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed848ba2-3663-4e6c-82dd-a27d95a06317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d28d7260-9e8c-4d6a-ab9c-a606ca9fee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_vocab_to_db(processed_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98fef90c-5a88-465e-9df1-0e1e57480fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_vocab_to_db(processed_docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d7af4c-274a-4919-a6f0-e75cf0cfd109",
   "metadata": {},
   "source": [
    "### Relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df0bcc3-81ca-4f01-831c-5cb82d7fdfa3",
   "metadata": {},
   "source": [
    "Les 3 types de relations dans le graphe, ce qu'elles connectent et ce qu'elles contiennent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71851b-a31c-4225-9668-76d7ac2e711a",
   "metadata": {},
   "source": [
    "phrases aux docs :\n",
    "- faire un call sur les phrases de la db \n",
    "- clé from = la clé de chaque phrase \n",
    "- clé to = la première partie de la clé "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cb76a79-fe26-400f-9305-6048d0bdcd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_keys = pd.Series(list(db.aql.execute('''\n",
    "                        FOR doc in sentences\n",
    "                        return doc._key\n",
    "                        ''')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cca4018b-bf4d-410e-b676-364d9883dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_number_for_sentences = sentences_keys.str.extract('(\\d+)')[0]\n",
    "sentences_number = sentences_keys.str.extract('\\D+\\d+\\D+(\\d+)')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "802c89c6-0961-4bfa-b0e1-595b07c4795b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       doc0sent0\n",
       "1       doc0sent1\n",
       "2      doc0sent10\n",
       "3      doc0sent11\n",
       "4      doc0sent13\n",
       "          ...    \n",
       "604    doc1sent94\n",
       "605    doc1sent95\n",
       "606    doc1sent96\n",
       "607    doc1sent97\n",
       "608    doc1sent99\n",
       "Length: 609, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eb3654-3ab1-42c0-8aab-098ea84b82f3",
   "metadata": {},
   "source": [
    "is_from :\n",
    "- les phrases aux documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0a652de-f45f-4161-874d-266cd216b383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': False,\n",
       " 'created': 609,\n",
       " 'errors': 0,\n",
       " 'empty': 0,\n",
       " 'updated': 0,\n",
       " 'ignored': 0,\n",
       " 'details': []}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_is_from_sent_doc_to_insert = []\n",
    "for sentence_key, doc_number, sentence_number in zip(sentences_keys.values,doc_number_for_sentences,sentences_number.values):\n",
    "    dict_is_from_sent_doc_to_insert.append({'_from':sentence_key,\n",
    "                                            '_to':f'doc{doc_number}',\n",
    "                                            'sentence_number': sentence_number,\n",
    "                                            'type':'sentToDoc'})\n",
    "db.collection('is_from').import_bulk(dict_is_from_sent_doc_to_insert,\n",
    "                                     from_prefix='sentences/',\n",
    "                                     to_prefix='docs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e931051c-e39f-4a20-9b2e-672b9152dd57",
   "metadata": {},
   "source": [
    "is_from :\n",
    "- les tokens aux phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72783cc-9582-40cb-b095-f33d754898e5",
   "metadata": {},
   "source": [
    "Extraction d'une forme tabulaire de la structure syntagmatique.\n",
    "  - Le processus est itéré sur l'ensemble des phrases du document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "069bb01f-f55d-4888-9fec-f51d09613031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dependancy_df(processed_text,doc_number):\n",
    "    token_text, token_lemma, token_dep, token_head_text, token_head_pos, sentence_number = [], [], [], [], [], []\n",
    "\n",
    "    for count, sentence in enumerate(processed_text.sents):\n",
    "        for token in sentence:\n",
    "            if not token.is_punct and not token.is_stop and not token.is_space and not token.is_digit:\n",
    "                token_text.append(token.text)\n",
    "                token_lemma.append(token.lemma_)\n",
    "                token_dep.append(token.dep_), \n",
    "                token_head_text.append(token.head.text), \n",
    "                token_head_pos.append( token.head.pos_)\n",
    "                sentence_number.append(f'doc{doc_number}sent{count}')\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame({'token':token_text,\n",
    "                       'lemma':token_lemma,\n",
    "                       'dep':token_dep,\n",
    "                       'head_text':token_head_text,\n",
    "                       'head_pos':token_head_pos,\n",
    "                       'sentence_number':sentence_number})   \n",
    "    df = df[df['head_pos']!=\"SPACE\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0b610dd-85a7-4c19-a3bf-59006d2358f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>dep</th>\n",
       "      <th>head_text</th>\n",
       "      <th>head_pos</th>\n",
       "      <th>sentence_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faire</td>\n",
       "      <td>faire</td>\n",
       "      <td>dep</td>\n",
       "      <td>durer</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inutilement</td>\n",
       "      <td>inutilement</td>\n",
       "      <td>advmod</td>\n",
       "      <td>durer</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>durer</td>\n",
       "      <td>durer</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>vais</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suspense</td>\n",
       "      <td>suspense</td>\n",
       "      <td>obj</td>\n",
       "      <td>durer</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>réponse</td>\n",
       "      <td>réponse</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>est</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>évoluer</td>\n",
       "      <td>évoluer</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>peuvent</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>cesse</td>\n",
       "      <td>cesse</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>jamais</td>\n",
       "      <td>jamais</td>\n",
       "      <td>advmod</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>étonner</td>\n",
       "      <td>étonner</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>intéresse</td>\n",
       "      <td>intéresse</td>\n",
       "      <td>conj</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           token        lemma     dep head_text head_pos sentence_number\n",
       "0          faire        faire     dep     durer     VERB       doc0sent0\n",
       "1    inutilement  inutilement  advmod     durer     VERB       doc0sent0\n",
       "2          durer        durer   xcomp      vais     VERB       doc0sent0\n",
       "3       suspense     suspense     obj     durer     VERB       doc0sent0\n",
       "4        réponse      réponse   nsubj       est     VERB       doc0sent0\n",
       "..           ...          ...     ...       ...      ...             ...\n",
       "106      évoluer      évoluer   xcomp   peuvent     VERB      doc0sent13\n",
       "107        cesse        cesse    ROOT     cesse     VERB      doc0sent14\n",
       "108       jamais       jamais  advmod     cesse     VERB      doc0sent14\n",
       "109      étonner      étonner   xcomp     cesse     VERB      doc0sent14\n",
       "110    intéresse    intéresse    conj     cesse     VERB      doc0sent14\n",
       "\n",
       "[110 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependancy_df = create_dependancy_df(processed_docs[0],0) \n",
    "dependancy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a0b37d7-ecb2-4de1-b213-21b03cc37b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faire</td>\n",
       "      <td>token0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inutilement</td>\n",
       "      <td>token1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>durer</td>\n",
       "      <td>token2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suspense</td>\n",
       "      <td>token3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>réponse</td>\n",
       "      <td>token4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2855</th>\n",
       "      <td>florence</td>\n",
       "      <td>token2856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>lefresne</td>\n",
       "      <td>token2857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2857</th>\n",
       "      <td>vecteurs</td>\n",
       "      <td>token2858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2858</th>\n",
       "      <td>structurelle</td>\n",
       "      <td>token2859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2859</th>\n",
       "      <td>juillet</td>\n",
       "      <td>token2860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2860 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word        key\n",
       "0            faire     token0\n",
       "1      inutilement     token1\n",
       "2            durer     token2\n",
       "3         suspense     token3\n",
       "4          réponse     token4\n",
       "...            ...        ...\n",
       "2855      florence  token2856\n",
       "2856      lefresne  token2857\n",
       "2857      vecteurs  token2858\n",
       "2858  structurelle  token2859\n",
       "2859       juillet  token2860\n",
       "\n",
       "[2860 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_table_tokens_in_db = get_vocab_in_db()[0]\n",
    "vocab_table_tokens_in_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb4c00d5-cda3-4390-b118-69a7030a52ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_from_sentence_table = dependancy_df.merge(vocab_table_tokens_in_db, \n",
    "                                                        left_on='token', \n",
    "                                                        right_on='word', sort = 'outer')\\\n",
    "                                                        .rename(columns={'key':'token_key'})\\\n",
    "                                                        .drop('word',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3760dba-26c3-486d-bade-972e3a4352b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>dep</th>\n",
       "      <th>head_text</th>\n",
       "      <th>head_pos</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>token_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aberration</td>\n",
       "      <td>aberration</td>\n",
       "      <td>obl:mod</td>\n",
       "      <td>aime</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent5</td>\n",
       "      <td>token33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aborder</td>\n",
       "      <td>aborder</td>\n",
       "      <td>acl</td>\n",
       "      <td>manière</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>doc0sent13</td>\n",
       "      <td>token85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aime</td>\n",
       "      <td>aime</td>\n",
       "      <td>conj</td>\n",
       "      <td>fait</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent5</td>\n",
       "      <td>token35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air</td>\n",
       "      <td>air</td>\n",
       "      <td>obj</td>\n",
       "      <td>donne</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent1</td>\n",
       "      <td>token13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ans</td>\n",
       "      <td>an</td>\n",
       "      <td>obl:mod</td>\n",
       "      <td>commence</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent9</td>\n",
       "      <td>token61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>vêtement</td>\n",
       "      <td>vêtement</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>vêtement</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>doc0sent9</td>\n",
       "      <td>token45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>vêtement</td>\n",
       "      <td>vêtement</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>peut</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent9</td>\n",
       "      <td>token45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>vêtement</td>\n",
       "      <td>vêtement</td>\n",
       "      <td>obj</td>\n",
       "      <td>aborder</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent13</td>\n",
       "      <td>token45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>étonner</td>\n",
       "      <td>étonner</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent14</td>\n",
       "      <td>token90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>évoluer</td>\n",
       "      <td>évoluer</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>peuvent</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent13</td>\n",
       "      <td>token87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          token       lemma      dep head_text head_pos sentence_number  \\\n",
       "0    aberration  aberration  obl:mod      aime     VERB       doc0sent5   \n",
       "1       aborder     aborder      acl   manière     NOUN      doc0sent13   \n",
       "2          aime        aime     conj      fait     VERB       doc0sent5   \n",
       "3           air         air      obj     donne     VERB       doc0sent1   \n",
       "4           ans          an  obl:mod  commence     VERB       doc0sent9   \n",
       "..          ...         ...      ...       ...      ...             ...   \n",
       "105    vêtement    vêtement     ROOT  vêtement     NOUN       doc0sent9   \n",
       "106    vêtement    vêtement    nsubj      peut     VERB       doc0sent9   \n",
       "107    vêtement    vêtement      obj   aborder     VERB      doc0sent13   \n",
       "108     étonner     étonner    xcomp     cesse     VERB      doc0sent14   \n",
       "109     évoluer     évoluer    xcomp   peuvent     VERB      doc0sent13   \n",
       "\n",
       "    token_key  \n",
       "0     token33  \n",
       "1     token85  \n",
       "2     token35  \n",
       "3     token13  \n",
       "4     token61  \n",
       "..        ...  \n",
       "105   token45  \n",
       "106   token45  \n",
       "107   token45  \n",
       "108   token90  \n",
       "109   token87  \n",
       "\n",
       "[110 rows x 7 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_from_sentence_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93298775-cc20-4e9c-9ac9-10f53ceffc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_from_sentence_table = dependancy_df.merge(vocab_table_tokens_in_db, \n",
    "                                                        left_on='token', \n",
    "                                                        right_on='word')\\\n",
    "                                                        .rename(columns={'key':'token_key'})\\\n",
    "                                                        .drop('word',axis=1)\n",
    "\n",
    "dependancy_table_for_insert = token_from_sentence_table.merge(vocab_table_tokens_in_db,\n",
    "                                                        left_on='head_text',\n",
    "                                                        right_on='word')\\\n",
    "                                                        .rename(columns={'key':'head_text_key'})\\\n",
    "                                                        .drop('word',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb84ffd2-ae64-4f64-b1fb-b18e977ce592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': False,\n",
       " 'created': 110,\n",
       " 'errors': 0,\n",
       " 'empty': 0,\n",
       " 'updated': 0,\n",
       " 'ignored': 0,\n",
       " 'details': []}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_is_from_sent_token_to_insert = []\n",
    "for token_key,sentence_number in zip(token_from_sentence_table['token_key'],token_from_sentence_table['sentence_number'] ):\n",
    "    dict_is_from_sent_token_to_insert.append({'_from':token_key,\n",
    "                                            '_to':sentence_number})\n",
    "db.collection('is_from').import_bulk(dict_is_from_sent_token_to_insert,\n",
    "                                     from_prefix='tokens/',\n",
    "                                     to_prefix='sentences/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "306e097a-c2c0-441e-9783-a5907fa2888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_dependancies(dependancy_df_item):\n",
    "    token_from_sentence_table = dependancy_df.merge(vocab_table_tokens_in_db, \n",
    "                                                        left_on='token', \n",
    "                                                        right_on='word', sort = 'outer')\\\n",
    "                                                        .rename(columns={'key':'token_key'})\\\n",
    "                                                        .drop('word',axis=1)\n",
    "    token_from_sentence_table = dependancy_df.merge(vocab_table_tokens_in_db, \n",
    "                                                        left_on='token', \n",
    "                                                        right_on='word')\\\n",
    "                                                        .rename(columns={'key':'token_key'})\\\n",
    "                                                        .drop('word',axis=1)\n",
    "\n",
    "    dependancy_table_for_insert = token_from_sentence_table.merge(vocab_table_tokens_in_db,\n",
    "                                                        left_on='head_text',\n",
    "                                                        right_on='word')\\\n",
    "                                                        .rename(columns={'key':'head_text_key'})\\\n",
    "                                                        .drop('word',axis=1)\n",
    "    dict_is_from_sent_token_to_insert = []\n",
    "    for token_key,sentence_number in zip(token_from_sentence_table['token_key'],token_from_sentence_table['sentence_number'] ):\n",
    "        dict_is_from_sent_token_to_insert.append({'_from':token_key,\n",
    "                                                '_to':sentence_number})\n",
    "    db.collection('is_from').import_bulk(dict_is_from_sent_token_to_insert,\n",
    "                                         from_prefix='tokens/',\n",
    "                                         to_prefix='sentences/')\n",
    "    \n",
    "    dict_syntagmatic_link_to_insert = []\n",
    "    for head_text_key, token_key, dep_relation, head_pos_tag, sentence_number in zip(dependancy_table_for_insert['head_text_key'],\n",
    "                                                                                     dependancy_table_for_insert['token_key'],\n",
    "                                                                                     dependancy_table_for_insert['dep'],\n",
    "                                                                                     dependancy_table_for_insert['head_pos'],\n",
    "                                                                                     dependancy_table_for_insert['sentence_number']):\n",
    "        dict_syntagmatic_link_to_insert.append({'_from':head_text_key,\n",
    "                                                '_to':token_key,\n",
    "                                                'dep_relation':dep_relation,\n",
    "                                                'head_pos_tag':head_pos_tag,\n",
    "                                                'from_sentence_number':sentence_number})\n",
    "    db.collection('syntagmatic_link').import_bulk(dict_syntagmatic_link_to_insert,\n",
    "                                                 from_prefix='tokens/',\n",
    "                                                 to_prefix='tokens/')\n",
    "    contracts_to_table = dependancy_table_for_insert.merge(lemmas_in_db, left_on='lemma', right_on='word')\\\n",
    "                        .rename(columns={'key':'lemma_key'})\\\n",
    "                        .drop('word',axis=1)\n",
    "    dict_contracts_to = []\n",
    "    for token_key, lemma_key, sentence_number in zip(contracts_to_table['token_key'],\n",
    "                                                     contracts_to_table['lemma_key'],\n",
    "                                                     contracts_to_table['sentence_number']):\n",
    "\n",
    "        dict_contracts_to.append({'_from':token_key,\n",
    "                                 '_to':lemma_key,\n",
    "                                 'sentence_number':sentence_number})\n",
    "\n",
    "    db.collection('contracts_to').import_bulk(dict_contracts_to,\n",
    "                                                 from_prefix='tokens/',\n",
    "                                                 to_prefix='lemmas/')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17299a75-99a9-4815-85a3-c371481f08fc",
   "metadata": {},
   "source": [
    "syntagmatic_link :\n",
    "- les tokens aux tokens \n",
    "    - le type de relation grammaticale\n",
    "    - l'ID de la phrase contenant cette relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b1222-3b8b-417c-b2d2-fe3821c30601",
   "metadata": {},
   "source": [
    "Merge des tokens avec les id correspondants dans la db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd450742-8fb0-4608-9f00-51d12f615349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': False,\n",
       " 'created': 84,\n",
       " 'errors': 0,\n",
       " 'empty': 0,\n",
       " 'updated': 0,\n",
       " 'ignored': 0,\n",
       " 'details': []}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_syntagmatic_link_to_insert = []\n",
    "for head_text_key, token_key, dep_relation, head_pos_tag, sentence_number in zip(dependancy_table_for_insert['head_text_key'],\n",
    "                                                                                 dependancy_table_for_insert['token_key'],\n",
    "                                                                                 dependancy_table_for_insert['dep'],\n",
    "                                                                                 dependancy_table_for_insert['head_pos'],\n",
    "                                                                                 dependancy_table_for_insert['sentence_number']):\n",
    "    dict_syntagmatic_link_to_insert.append({'_from':head_text_key,\n",
    "                                            '_to':token_key,\n",
    "                                            'dep_relation':dep_relation,\n",
    "                                            'head_pos_tag':head_pos_tag,\n",
    "                                            'from_sentence_number':sentence_number})\n",
    "db.collection('syntagmatic_link').import_bulk(dict_syntagmatic_link_to_insert,\n",
    "                                             from_prefix='tokens/',\n",
    "                                             to_prefix='tokens/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93900680-0645-474c-9a78-13fc34cf7166",
   "metadata": {},
   "source": [
    "contracts_to :\n",
    "- Les tokens aux lemmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4436231-35c4-4777-818e-d6966c22ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_in_db = get_vocab_in_db()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bfeacb02-04cf-4dc2-af20-8af8c64d3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_to_table = dependancy_table_for_insert.merge(lemmas_in_db, left_on='lemma', right_on='word')\\\n",
    "                        .rename(columns={'key':'lemma_key'})\\\n",
    "                        .drop('word',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d4a7295-366d-4dc0-89e5-b9a042d18de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>dep</th>\n",
       "      <th>head_text</th>\n",
       "      <th>head_pos</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>token_key</th>\n",
       "      <th>head_text_key</th>\n",
       "      <th>lemma_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faire</td>\n",
       "      <td>faire</td>\n",
       "      <td>dep</td>\n",
       "      <td>durer</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "      <td>token0</td>\n",
       "      <td>token2</td>\n",
       "      <td>lemma0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faire</td>\n",
       "      <td>faire</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent7</td>\n",
       "      <td>token0</td>\n",
       "      <td>token0</td>\n",
       "      <td>lemma0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inutilement</td>\n",
       "      <td>inutilement</td>\n",
       "      <td>advmod</td>\n",
       "      <td>durer</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "      <td>token1</td>\n",
       "      <td>token2</td>\n",
       "      <td>lemma1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suspense</td>\n",
       "      <td>suspense</td>\n",
       "      <td>obj</td>\n",
       "      <td>durer</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent0</td>\n",
       "      <td>token3</td>\n",
       "      <td>token2</td>\n",
       "      <td>lemma3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>peut-être</td>\n",
       "      <td>peut-être</td>\n",
       "      <td>advmod</td>\n",
       "      <td>faire</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent7</td>\n",
       "      <td>token38</td>\n",
       "      <td>token0</td>\n",
       "      <td>lemma38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>aborder</td>\n",
       "      <td>aborder</td>\n",
       "      <td>acl</td>\n",
       "      <td>manière</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>doc0sent13</td>\n",
       "      <td>token85</td>\n",
       "      <td>token84</td>\n",
       "      <td>lemma85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>cesse</td>\n",
       "      <td>cesse</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent14</td>\n",
       "      <td>token88</td>\n",
       "      <td>token88</td>\n",
       "      <td>lemma88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>jamais</td>\n",
       "      <td>jamais</td>\n",
       "      <td>advmod</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent14</td>\n",
       "      <td>token89</td>\n",
       "      <td>token88</td>\n",
       "      <td>lemma89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>étonner</td>\n",
       "      <td>étonner</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent14</td>\n",
       "      <td>token90</td>\n",
       "      <td>token88</td>\n",
       "      <td>lemma90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>intéresse</td>\n",
       "      <td>intéresse</td>\n",
       "      <td>conj</td>\n",
       "      <td>cesse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>doc0sent14</td>\n",
       "      <td>token91</td>\n",
       "      <td>token88</td>\n",
       "      <td>lemma91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          token        lemma     dep head_text head_pos sentence_number  \\\n",
       "0         faire        faire     dep     durer     VERB       doc0sent0   \n",
       "1         faire        faire    ROOT     faire     VERB       doc0sent7   \n",
       "2   inutilement  inutilement  advmod     durer     VERB       doc0sent0   \n",
       "3      suspense     suspense     obj     durer     VERB       doc0sent0   \n",
       "4     peut-être    peut-être  advmod     faire     VERB       doc0sent7   \n",
       "..          ...          ...     ...       ...      ...             ...   \n",
       "79      aborder      aborder     acl   manière     NOUN      doc0sent13   \n",
       "80        cesse        cesse    ROOT     cesse     VERB      doc0sent14   \n",
       "81       jamais       jamais  advmod     cesse     VERB      doc0sent14   \n",
       "82      étonner      étonner   xcomp     cesse     VERB      doc0sent14   \n",
       "83    intéresse    intéresse    conj     cesse     VERB      doc0sent14   \n",
       "\n",
       "   token_key head_text_key lemma_key  \n",
       "0     token0        token2    lemma0  \n",
       "1     token0        token0    lemma0  \n",
       "2     token1        token2    lemma1  \n",
       "3     token3        token2    lemma3  \n",
       "4    token38        token0   lemma38  \n",
       "..       ...           ...       ...  \n",
       "79   token85       token84   lemma85  \n",
       "80   token88       token88   lemma88  \n",
       "81   token89       token88   lemma89  \n",
       "82   token90       token88   lemma90  \n",
       "83   token91       token88   lemma91  \n",
       "\n",
       "[84 rows x 9 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contracts_to_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ba4fa4a-b5a1-4342-b56d-a304c8734861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': False,\n",
       " 'created': 84,\n",
       " 'errors': 0,\n",
       " 'empty': 0,\n",
       " 'updated': 0,\n",
       " 'ignored': 0,\n",
       " 'details': []}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_contracts_to = []\n",
    "for token_key, lemma_key, sentence_number in zip(contracts_to_table['token_key'],\n",
    "                                                 contracts_to_table['lemma_key'],\n",
    "                                                 contracts_to_table['sentence_number']):\n",
    "    \n",
    "    dict_contracts_to.append({'_from':token_key,\n",
    "                             '_to':lemma_key,\n",
    "                             'sentence_number':sentence_number})\n",
    "    \n",
    "db.collection('contracts_to').import_bulk(dict_contracts_to,\n",
    "                                             from_prefix='tokens/',\n",
    "                                             to_prefix='lemmas/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7f9411-7ec6-44f3-87d8-8f1dd14755f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6714ae7e-fccc-49d0-84dc-d0e5bf9319ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da43c033-cdaf-4fc0-915b-c8d92393bb39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eb2449-b5f1-4ebf-838b-b8df5778af8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a108809c-d4ec-41b0-8295-86c245b5d7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced1d7ac-a9a3-416e-a066-5a242892a170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
